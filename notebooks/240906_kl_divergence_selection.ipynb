{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mask_learning_case(nn.Module):\n",
    "    def __init__(self, n_mask=10, n_output=2, gene_cnt=550):\n",
    "        super().__init__()\n",
    "        self.n_mask = n_mask\n",
    "        self.n_output = n_output\n",
    "        self.gene_cnt = gene_cnt\n",
    "        self.logits = nn.Parameter(torch.randn(self.n_mask, self.gene_cnt))\n",
    "        self.lin1 = nn.Linear(self.n_mask, self.n_output)\n",
    "\n",
    "    def forward(self, x, temp):\n",
    "        mask = F.gumbel_softmax(self.logits, tau=temp, hard=True)  # samples 1 for each row\n",
    "        mask = mask[:, 0]  # 0th idx is \"selected\" and 1th idx is \"not selectd\"\n",
    "        # print(self.logits.shape)\n",
    "        # print(mask.shape, x.shape)\n",
    "\n",
    "        y_pred = self.lin1(\n",
    "            mask.reshape(\n",
    "                -1,\n",
    "            )\n",
    "            * x\n",
    "        )\n",
    "\n",
    "        return y_pred, [self.logits]\n",
    "\n",
    "    def softmax(self):\n",
    "        return F.softmax(self.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this first case, we use each gumbel_softmask to select a unique feature (# gumbel-softmax = number of features to choose). concrete distribution chooses 1 out of m (where m is total # of features)\n",
    "# in the second cae, we use a gumbel_softmask for each feature (# gumbel-softmax - total # of features): yes vs no. concrete distribution chooses 1 out of 2\n",
    "class mask_learning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logits1 = nn.Parameter(torch.randn(3))\n",
    "        self.logits2 = nn.Parameter(torch.randn(3))\n",
    "        self.lin1 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, x, temp):\n",
    "        mask1 = F.gumbel_softmax(self.logits1, tau=temp, hard=True)\n",
    "        mask2 = F.gumbel_softmax(self.logits2, tau=temp, hard=True)\n",
    "        mask = (\n",
    "            mask1 + mask2\n",
    "        )  # masks are used to select 2 indices out of 3; we want them to select the first and second.\n",
    "        mask = torch.clamp(\n",
    "            mask, min=0, max=1\n",
    "        )  # clamping is done so that if both masks pick the same element, 2 becomes 1\n",
    "        y_pred = self.lin1(\n",
    "            mask * x\n",
    "        )  # elementwise multiplication, mask is broadcasted for all rows of x. self.lin1 converts 3 dim into 2 dim. We want lin1 to just select the positive elements from mask * x.\n",
    "        # but lin1 could just learn a function to map an \"incorrect\" mask*x to the correct y labels (since task is not complex either)\n",
    "        return y_pred, [self.logits1, self.logits2]\n",
    "\n",
    "    def softmax(self):\n",
    "        return F.softmax(self.logits1), F.softmax(self.logits2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 3)\n",
    "y = x[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits1 tensor([-1.0162, -1.0681,  0.0481])\n",
      "logits2 tensor([ 1.1409,  0.1846, -0.3736])\n",
      "lin1.weight tensor([[ 0.1511,  0.1861,  0.5604],\n",
      "        [ 0.3632, -0.0440, -0.1327]])\n",
      "lin1.bias tensor([-0.4298, -0.0884])\n"
     ]
    }
   ],
   "source": [
    "model = mask_learning()\n",
    "print(\"initial distribution\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using MAE loss, distribution converges to target\n",
    "for epoch in range(101):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred, logits = model(x, 1)\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits1 tensor([0.0624, 1.1732, 0.1909])\n",
      "logits2 tensor([-0.4508,  0.3173, -1.1861])\n",
      "lin1.weight tensor([[-0.3245,  0.4395, -0.5165],\n",
      "        [-0.0170, -0.3961,  0.1580]])\n",
      "lin1.bias tensor([-0.1179, -0.2119])\n"
     ]
    }
   ],
   "source": [
    "# Temperature annealing\n",
    "\n",
    "model2 = mask_learning()\n",
    "print(\"initial distribution\")\n",
    "for name, param in model2.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters())\n",
    "\n",
    "temp = 10\n",
    "max_epoch = 101\n",
    "\n",
    "\n",
    "def temp_schedule(epoch):\n",
    "    return 10 * (1 - epoch / max_epoch) + 1e-5\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    optimizer2.zero_grad()\n",
    "    y_pred, logits = model2(x, temp_schedule(epoch))\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    loss.backward()\n",
    "    optimizer2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_input  == n_mask\n",
    "f\n",
    "\n",
    "\n",
    "class mask_learning_case_2(nn.Module):\n",
    "    def __init__(self, n_mask=3, n_output=2):\n",
    "        super().__init__()\n",
    "        self.n_mask = n_mask\n",
    "        self.n_output = n_output\n",
    "        self.logits = nn.Parameter(torch.randn(self.n_mask, 2))\n",
    "        self.lin1 = nn.Linear(self.n_mask, self.n_output)\n",
    "\n",
    "    def forward(self, x, temp):\n",
    "        mask = F.gumbel_softmax(self.logits, tau=temp, hard=True)  # samples 1 for each row\n",
    "        mask = mask[:, 0]  # 0th idx is \"selected\" and 1th idx is \"not selectd\"\n",
    "        # print(self.logits.shape)\n",
    "        print(mask.shape, x.shape)\n",
    "\n",
    "        y_pred = self.lin1(\n",
    "            mask.reshape(\n",
    "                -1,\n",
    "            )\n",
    "            * x\n",
    "        )\n",
    "\n",
    "        return y_pred, [self.logits]\n",
    "\n",
    "    def softmax(self):\n",
    "        return F.softmax(self.logits, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits tensor([[ 0.9418,  0.3991],\n",
      "        [ 1.4392, -0.6753],\n",
      "        [ 0.4566, -1.4735]])\n",
      "lin1.weight tensor([[ 0.3486, -0.2031, -0.0548],\n",
      "        [-0.0998,  0.4586, -0.2397]])\n",
      "lin1.bias tensor([ 0.0207, -0.4857])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n",
      "torch.Size([3]) torch.Size([1000, 3])\n"
     ]
    }
   ],
   "source": [
    "# Temperature annealing\n",
    "\n",
    "model3 = mask_learning_case_2()\n",
    "print(\"initial distribution\")\n",
    "for name, param in model3.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer3 = torch.optim.Adam(model3.parameters())\n",
    "\n",
    "temp = 10\n",
    "max_epoch = 101\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    optimizer3.zero_grad()\n",
    "    y_pred, logits = model3(x, 0.1)\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    # if epoch % 1000 == 0:\n",
    "    #     with torch.no_grad():\n",
    "    #         print(model3.softmax())\n",
    "    #         print(float(loss))\n",
    "    loss.backward()\n",
    "    optimizer3.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
