{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mask_learning(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.logits1 = nn.Parameter(torch.randn(3))\n",
    "        self.logits2 = nn.Parameter(torch.randn(3))\n",
    "        self.lin1 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, x, temp):\n",
    "        mask1 = F.gumbel_softmax(self.logits1, tau=temp, hard=True)\n",
    "        mask2 = F.gumbel_softmax(self.logits2, tau=temp, hard=True)\n",
    "        mask = mask1 + mask2\n",
    "        mask = torch.clamp(mask, min=0, max=1)\n",
    "        y_pred = self.lin1(mask * x)\n",
    "\n",
    "        return y_pred, [self.logits1, self.logits2]\n",
    "    \n",
    "    def softmax(self):\n",
    "        return F.softmax(self.logits1), F.softmax(self.logits2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 3)\n",
    "y = x[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits1 tensor([ 0.1910, -0.5794, -1.2352])\n",
      "logits2 tensor([-1.0291, -1.2941,  2.6254])\n",
      "lin1.weight tensor([[-0.3464,  0.3120, -0.4000],\n",
      "        [ 0.0294, -0.3264, -0.0787]])\n",
      "lin1.bias tensor([-0.5049, -0.4018])\n"
     ]
    }
   ],
   "source": [
    "model = mask_learning(temp=1)\n",
    "print(\"initial distribution\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "loss_fn = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3487484/1567990288.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.logits1), F.softmax(self.logits2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.5872, 0.2717, 0.1411]), tensor([0.0247, 0.0190, 0.9563]))\n",
      "1.332318902015686\n",
      "(tensor([0.2416, 0.6363, 0.1220]), tensor([0.0148, 0.0291, 0.9560]))\n",
      "0.06507055461406708\n",
      "(tensor([0.1183, 0.7964, 0.0853]), tensor([0.0106, 0.0310, 0.9584]))\n",
      "0.45338794589042664\n",
      "(tensor([0.0710, 0.8758, 0.0532]), tensor([0.0090, 0.0297, 0.9614]))\n",
      "4.3489377276273444e-05\n",
      "(tensor([0.0483, 0.9126, 0.0391]), tensor([0.0076, 0.0278, 0.9646]))\n",
      "1.7782284089662426e-07\n",
      "(tensor([0.0348, 0.9362, 0.0291]), tensor([0.0067, 0.0244, 0.9690]))\n",
      "6.974802545300918e-06\n",
      "(tensor([0.0256, 0.9500, 0.0243]), tensor([0.0055, 0.0201, 0.9745]))\n",
      "1.86763300007442e-05\n",
      "(tensor([0.0197, 0.9607, 0.0196]), tensor([0.0047, 0.0177, 0.9776]))\n",
      "9.355631647167684e-08\n",
      "(tensor([0.0161, 0.9673, 0.0166]), tensor([0.0039, 0.0140, 0.9820]))\n",
      "7.748096322757192e-06\n",
      "(tensor([0.0127, 0.9737, 0.0136]), tensor([0.0033, 0.0117, 0.9850]))\n",
      "7.504349014197942e-06\n",
      "(tensor([0.0104, 0.9779, 0.0117]), tensor([0.0031, 0.0105, 0.9864]))\n",
      "9.683490134193562e-07\n"
     ]
    }
   ],
   "source": [
    "# Using MAE loss, distribution converges to target\n",
    "for epoch in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred, logits = model(x)\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if epoch % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            print(model.softmax())\n",
    "            print(float(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits1 tensor([0.5194, 0.3145, 0.9577])\n",
      "logits2 tensor([ 1.0598,  1.1816, -0.1711])\n",
      "lin1.weight tensor([[-0.1794, -0.1443, -0.1944],\n",
      "        [ 0.2465, -0.1393,  0.3290]])\n",
      "lin1.bias tensor([-0.4425,  0.1930])\n",
      "(tensor([0.2972, 0.2421, 0.4607]), tensor([0.4130, 0.4664, 0.1206]))\n",
      "1.2145395278930664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3487484/1963048758.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.logits1), F.softmax(self.logits2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.0897, 0.1742, 0.7361]), tensor([0.1811, 0.5337, 0.2852]))\n",
      "0.12098657339811325\n",
      "(tensor([0.0320, 0.2499, 0.7181]), tensor([0.0543, 0.7009, 0.2448]))\n",
      "0.005308471620082855\n",
      "(tensor([0.0133, 0.2443, 0.7424]), tensor([0.0229, 0.7195, 0.2577]))\n",
      "0.00020985625451430678\n",
      "(tensor([0.0061, 0.2397, 0.7542]), tensor([0.0104, 0.7277, 0.2620]))\n",
      "0.00015841623826418072\n",
      "(tensor([0.0030, 0.2373, 0.7597]), tensor([0.0050, 0.7383, 0.2567]))\n",
      "0.0001733555836835876\n",
      "(tensor([0.0017, 0.2462, 0.7522]), tensor([0.0026, 0.7602, 0.2372]))\n",
      "0.00017947357264347374\n",
      "(tensor([0.0009, 0.2253, 0.7737]), tensor([0.0015, 0.7550, 0.2435]))\n",
      "0.5191536545753479\n",
      "(tensor([6.3644e-04, 2.2100e-01, 7.7836e-01]), tensor([0.0010, 0.7833, 0.2157]))\n",
      "5.352523294277489e-05\n",
      "(tensor([5.0899e-04, 1.8282e-01, 8.1667e-01]), tensor([7.8806e-04, 7.9873e-01, 2.0048e-01]))\n",
      "5.673144187312573e-05\n"
     ]
    }
   ],
   "source": [
    "# Temperature annealing\n",
    "\n",
    "model2 = mask_learning()\n",
    "print(\"initial distribution\")\n",
    "for name, param in model2.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters())\n",
    "\n",
    "temp = 10\n",
    "max_epoch = 10000\n",
    "\n",
    "def temp_schedule(epoch):\n",
    "    return 10 * (1 - epoch / max_epoch) + 1e-5\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    \n",
    "    optimizer2.zero_grad()\n",
    "    y_pred, logits = model2(x, temp_schedule(epoch))\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if epoch % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            print(model2.softmax())\n",
    "            print(float(loss))\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_input  == n_mask\n",
    "class mask_learning_case_2(nn.Module):\n",
    "    def __init__(self, n_mask = 3, n_output = 2):\n",
    "        super().__init__()\n",
    "        self.n_mask = n_mask\n",
    "        self.n_output = n_output\n",
    "        self.logits = nn.Parameter(torch.randn(self.n_mask, 2))\n",
    "        self.lin1 = nn.Linear(self.n_mask, self.n_output)\n",
    "\n",
    "    def forward(self, x, temp):\n",
    "        mask = F.gumbel_softmax(self.logits, tau=temp, hard=True) #samples 1 for each row\n",
    "        mask = mask[:, 0] # 0th idx is \"selected\" and 1th idx is \"not selectd\"\n",
    "        # print(self.logits.shape)\n",
    "        # print(mask.shape, x.shape)\n",
    "\n",
    "        y_pred = self.lin1(mask.reshape(-1, ) * x)\n",
    "\n",
    "        return y_pred, [self.logits]\n",
    "    \n",
    "    def softmax(self):\n",
    "        return F.softmax(self.logits, dim = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits tensor([[-0.3636, -0.8735],\n",
      "        [-0.3875,  0.7121],\n",
      "        [-1.0806, -0.2381]])\n",
      "lin1.weight tensor([[-0.3462, -0.2087, -0.2830],\n",
      "        [-0.0008,  0.1352,  0.3754]])\n",
      "lin1.bias tensor([0.3326, 0.5095])\n",
      "tensor([[0.6248, 0.3752],\n",
      "        [0.2498, 0.7502],\n",
      "        [0.3010, 0.6990]])\n",
      "1.2047051191329956\n",
      "tensor([[0.5403, 0.4597],\n",
      "        [0.2996, 0.7004],\n",
      "        [0.7321, 0.2679]])\n",
      "0.2965875566005707\n",
      "tensor([[0.5423, 0.4577],\n",
      "        [0.8787, 0.1213],\n",
      "        [0.8682, 0.1318]])\n",
      "0.012684086337685585\n",
      "tensor([[0.5424, 0.4576],\n",
      "        [0.9431, 0.0569],\n",
      "        [0.9167, 0.0833]])\n",
      "1.99364385480294e-05\n",
      "tensor([[0.5424, 0.4576],\n",
      "        [0.9606, 0.0394],\n",
      "        [0.9483, 0.0517]])\n",
      "2.014121309912298e-05\n",
      "tensor([[0.5422, 0.4578],\n",
      "        [0.9722, 0.0278],\n",
      "        [0.9654, 0.0346]])\n",
      "2.79874602711061e-05\n",
      "tensor([[0.5415, 0.4585],\n",
      "        [0.9769, 0.0231],\n",
      "        [0.9741, 0.0259]])\n",
      "2.3776821933552128e-07\n",
      "tensor([[0.5403, 0.4597],\n",
      "        [0.9819, 0.0181],\n",
      "        [0.9812, 0.0188]])\n",
      "9.519861343676439e-09\n",
      "tensor([[0.5373, 0.4627],\n",
      "        [0.9854, 0.0146],\n",
      "        [0.9859, 0.0141]])\n",
      "8.108274940354931e-09\n",
      "tensor([[0.5259, 0.4741],\n",
      "        [0.9896, 0.0104],\n",
      "        [0.9887, 0.0113]])\n",
      "4.199216903089109e-07\n"
     ]
    }
   ],
   "source": [
    "# Temperature annealing\n",
    "\n",
    "model3 = mask_learning_case_2()\n",
    "print(\"initial distribution\")\n",
    "for name, param in model3.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer3 = torch.optim.Adam(model3.parameters())\n",
    "\n",
    "temp = 10\n",
    "max_epoch = 10000\n",
    "\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    \n",
    "    optimizer3.zero_grad()\n",
    "    y_pred, logits = model3(x, temp_schedule(epoch))\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if epoch % 1000 == 0:\n",
    "        with torch.no_grad():\n",
    "            print(model3.softmax())\n",
    "            print(float(loss))\n",
    "    loss.backward()\n",
    "    optimizer3.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[6.5783e-07, 1.0000e+00, 1.4398e-07],\n",
       "        [2.8003e-09, 2.0142e-09, 1.0000e+00]], requires_grad=True)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.lin1.weight"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
