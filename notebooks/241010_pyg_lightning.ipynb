{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GATv2Conv\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "from scipy.sparse import issparse\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "\n",
    "\n",
    "class AnnDataGraphDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Tabular AnnData dataset.\n",
    "\n",
    "    Args:\n",
    "        path (str): Path to the AnnData file.\n",
    "        keep_genes (list): List of genes to keep.\n",
    "        keep_cells (list): List of cells to keep.\n",
    "        spatial_coords (list): Anndata.obs columns for spatial coordinates (for regression)\n",
    "        cell_type (str): Anndata.obs column for cell types.\n",
    "        max_order (int): Maximum order of neighbors to consider.\n",
    "        d_threshold (float): Distance threshold (in mm) for considering neighbors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        path,\n",
    "        keep_genes=None,\n",
    "        keep_cells=None,\n",
    "        spatial_coords=[\"x_ccf\", \"y_ccf\", \"z_ccf\"],\n",
    "        cell_type=\"supertype\",\n",
    "        max_order=2,\n",
    "        d_threshold=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.path = path\n",
    "        adata = ad.read_h5ad(self.path)\n",
    "        assert \"connectivities\" in adata.obsp.keys(), \"Spatial connectivities not found. Run `sc.pp.neighbors` first.\"\n",
    "        assert \"distances\" in adata.obsp.keys(), \"Spatial distances not found. Run `sc.pp.neighbors` first.\"\n",
    "\n",
    "        # filter genes\n",
    "        if keep_genes is not None:\n",
    "            adata = adata[:, keep_genes].copy()\n",
    "        else:\n",
    "            keep_genes = get_non_blank_genes(adata)\n",
    "            adata = adata[:, keep_genes].copy()\n",
    "\n",
    "        # filter cells\n",
    "        if keep_cells is not None:\n",
    "            adata = adata[keep_cells, :].copy()\n",
    "\n",
    "        self.adata = adata\n",
    "        self.max_order = max_order\n",
    "        self.d_threshold = d_threshold\n",
    "\n",
    "        # create binary adjacency matrix without self-loops\n",
    "        adj = self.adata.obsp[\"connectivities\"].copy()\n",
    "        adj = adj.astype(bool).astype(int)\n",
    "        adj[self.adata.obsp[\"distances\"] > self.d_threshold] = 0\n",
    "        adj.setdiag(0)\n",
    "\n",
    "        # create adjacency matrices up to max_order\n",
    "        self.adj_matrices = {}\n",
    "        self.adj_matrices[1] = adj.copy()\n",
    "        if self.max_order > 1:\n",
    "            for i in range(2, self.max_order + 1):\n",
    "                self.adj_matrices[i] = adj.dot(self.adj_matrices[i - 1])\n",
    "\n",
    "        self.spatial_coords = spatial_coords\n",
    "        self.cell_type = cell_type\n",
    "        self.cell_type_list = adata.obs[cell_type].cat.categories.tolist()\n",
    "        self.cell_type_labelencoder = LabelEncoder()\n",
    "        self.cell_type_labelencoder.fit(self.cell_type_list)\n",
    "        self.data_issparse = issparse(adata.X)\n",
    "\n",
    "    def get_neighbors(self, idx):\n",
    "        nhood_idx = []\n",
    "        for i in range(1, self.max_order + 1):\n",
    "            nhood_idx.append(np.where(self.adj_matrices[i][idx, :].toarray().flatten())[0])\n",
    "        nhood_idx = np.concatenate(nhood_idx, axis=0)\n",
    "        nhood_idx = np.unique(np.concatenate([nhood_idx, [idx]]))\n",
    "        return nhood_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.adata.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get all neighbors\n",
    "        nhood_idx = self.get_neighbors(idx)\n",
    "        local_adj = self.adj_matrices[1][np.ix_(nhood_idx, nhood_idx)]\n",
    "        edgelist = np.array(local_adj.nonzero()).T\n",
    "\n",
    "        gene_exp = self.adata.X[nhood_idx, :]\n",
    "        if self.data_issparse:\n",
    "            gene_exp = gene_exp.toarray().astype(np.float32)\n",
    "        xyz = self.adata.obs.iloc[nhood_idx][self.spatial_coords].values.astype(np.float32)\n",
    "        celltype = self.cell_type_labelencoder.transform(self.adata.obs.iloc[nhood_idx][self.cell_type])\n",
    "        return gene_exp, edgelist, celltype\n",
    "\n",
    "\n",
    "def get_non_blank_genes(adata):\n",
    "    keep_genes = adata.var[~adata.var.index.str.startswith(\"Blank\")].index\n",
    "    return keep_genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def get_paths(verbose: bool = False) -> dict:\n",
    "    \"\"\"\n",
    "    Get custom paths from config.toml that is in the root directory.\n",
    "    \"\"\"\n",
    "\n",
    "    # get path of this file\n",
    "    root_path = Path(\"../\")\n",
    "    config_path = root_path / \"config.toml\"\n",
    "    if not config_path.exists():\n",
    "        raise FileNotFoundError(f\"Config file not found at {config_path}\")\n",
    "    config = toml.load(config_path)\n",
    "    config[\"package_root\"] = root_path\n",
    "    if verbose:\n",
    "        print(config)\n",
    "    return config\n",
    "\n",
    "\n",
    "def get_datetime(expname: str = \"\"):\n",
    "    datetime_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    if expname is None:\n",
    "        expname = datetime_str\n",
    "    else:\n",
    "        expname = f\"{datetime_str}_{expname}\"\n",
    "    return expname\n",
    "\n",
    "\n",
    "def get_adata(path: str):\n",
    "    adata = ad.read_h5ad(path)\n",
    "    adata.obsm[\"ccf\"] = np.concatenate(\n",
    "        (\n",
    "            np.expand_dims(np.array(adata.obs[\"x_ccf\"]), axis=1),\n",
    "            np.expand_dims(np.array(adata.obs[\"y_ccf\"]), axis=1),\n",
    "            np.expand_dims(np.array(adata.obs[\"z_ccf\"]), axis=1),\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "    adata.var.set_index(\"gene_symbol\", inplace=True, drop=False)\n",
    "\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = get_adata(\"../data/VISp_nhood.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0               0162 OB Dopa-Gaba_1\n",
       "1                0135 HPF CR Glut_1\n",
       "2                0135 HPF CR Glut_1\n",
       "3                0135 HPF CR Glut_1\n",
       "4        1149 CBX MLI Megf11 Gaba_1\n",
       "                    ...            \n",
       "61879                1193 Endo NN_1\n",
       "61880                1193 Endo NN_1\n",
       "61881                1193 Endo NN_1\n",
       "61882                1193 Endo NN_1\n",
       "61883                1193 Endo NN_1\n",
       "Name: supertype, Length: 61884, dtype: category\n",
       "Categories (126, object): ['0001 CLA-EPd-CTX Car3 Glut_1', '0002 CLA-EPd-CTX Car3 Glut_2', '0003 IT EP-CLA Glut_1', '0004 IT EP-CLA Glut_2', ..., '1196 Monocytes NN_1', '1197 DC NN_1', '1198 B cells NN_1', '1201 T cells NN_4']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.obs[\"supertype\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import toml\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import anndata as ad\n",
    "import lightning as L\n",
    "import torch\n",
    "from torch.utils.data import ConcatDataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "class AnnDataGraphDataModule(L.LightningDataModule):\n",
    "    def __init__(self, data_dir: None, file_names: list[str] = [\"VISp_nhood.h5ad\"], batch_size: int = 1):\n",
    "        super().__init__()\n",
    "        if data_dir is None:\n",
    "            data_dir = get_paths()[\"data_root\"]\n",
    "            # data_dir = \"../data/\"\n",
    "        self.adata_paths = [str(data_dir) + file_name for file_name in file_names]\n",
    "        for adata_path in self.adata_paths:\n",
    "            if not Path(adata_path).exists():\n",
    "                raise FileNotFoundError(f\"File not found: {adata_path}\")\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage: str):\n",
    "        self.adatas = []\n",
    "        for adata_path in self.adata_paths:\n",
    "            self.adatas.append(AnnDataGraphDataset(adata_path))\n",
    "        self.data_full = ConcatDataset(self.adatas)\n",
    "        self.data_train, self.data_test = random_split(\n",
    "            self.data_full, [0.8, 0.2], generator=torch.Generator().manual_seed(0)\n",
    "        )\n",
    "\n",
    "        if stage == \"fit\":\n",
    "            self.data_train, self.data_val = random_split(\n",
    "                self.data_train, [0.8, 0.2], generator=torch.Generator().manual_seed(1)\n",
    "            )\n",
    "\n",
    "        if stage == \"test\":  # Note: this is not the test set. Just a quick way to check the model through lightining.\n",
    "            _, self.data_test = random_split(self.data_full, [0.9, 0.1], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "        if stage == \"predict\":\n",
    "            self.data_predict = self.data_full\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.data_train, batch_size=self.batch_size, shuffle=True, pin_memory=True, num_workers=16)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.data_val, batch_size=self.batch_size, shuffle=False, pin_memory=True, num_workers=16)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.data_test, batch_size=self.batch_size, shuffle=False, pin_memory=True, num_workers=16)\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(self.data_predict, batch_size=self.batch_size, shuffle=False, pin_memory=True, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT3(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.conv1 = GATv2Conv(self.num_features, self.hidden_channels, heads=8, concat=False)\n",
    "        self.conv2 = GATv2Conv(self.hidden_channels, self.num_classes, heads=8, concat=False)\n",
    "        self.lin1 = nn.Linear(self.num_features, self.num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        residual1 = self.lin1(x)\n",
    "        out = self.conv1(x, edge_index)\n",
    "        out = out.relu()\n",
    "        out = self.dropout(out)\n",
    "        out = self.conv2(out, edge_index)\n",
    "        out = out + residual1\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchmetrics import MeanSquaredError\n",
    "from torchmetrics.classification import MulticlassAccuracy\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "\n",
    "class LitGNNv0(L.LightningModule):\n",
    "    def __init__(self, input_dim, hidden_dim, n_labels, weight_mse=1.0, weight_ce=1.0):\n",
    "        super(LitGNNv0, self).__init__()\n",
    "\n",
    "        self.weight_mse = weight_mse\n",
    "        self.weight_ce = weight_ce\n",
    "\n",
    "        self.GAT = GAT3(hidden_channels=32, num_features=input_dim, num_classes=n_labels)\n",
    "\n",
    "        # losses\n",
    "        self.loss_ce = nn.CrossEntropyLoss()\n",
    "\n",
    "        # metrics\n",
    "        self.metric_overall_acc = MulticlassAccuracy(\n",
    "            num_classes=n_labels, top_k=1, average=\"weighted\", multidim_average=\"global\"\n",
    "        )\n",
    "        self.metric_macro_acc = MulticlassAccuracy(\n",
    "            num_classes=n_labels, top_k=1, average=\"macro\", multidim_average=\"global\"\n",
    "        )\n",
    "        self.metric_multiclass_acc = MulticlassAccuracy(\n",
    "            num_classes=n_labels, top_k=1, average=None, multidim_average=\"global\"\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        celltype = self.GAT(x, edge_index)\n",
    "        return celltype\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # for GNN, batch size should be 1, and there isn't a batch dimension.\n",
    "        gene_exp, edgelist, celltype = batch\n",
    "        gene_exp = gene_exp.squeeze(dim=0)\n",
    "        edgelist = edgelist.squeeze(dim=0).T\n",
    "        celltype = celltype.squeeze(dim=0)\n",
    "\n",
    "        celltype_pred = self.forward(gene_exp, edgelist)\n",
    "\n",
    "        # Calculate losses\n",
    "        total_loss = self.loss_ce(celltype_pred, celltype.squeeze())\n",
    "\n",
    "        # Log losses\n",
    "        self.log(\"train_total_loss\", total_loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        # Calculate metrics\n",
    "        train_overall_acc = self.metric_overall_acc(preds=celltype_pred, target=celltype.reshape(-1))\n",
    "        train_macro_acc = self.metric_macro_acc(preds=celltype_pred, target=celltype.reshape(-1))\n",
    "\n",
    "        # Log metrics\n",
    "        self.log(\"train_overall_acc\", train_overall_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"train_macro_acc\", train_macro_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # for GNN, batch size should be 1, and there isn't a batch dimension.\n",
    "        gene_exp, edgelist, celltype = batch\n",
    "        gene_exp = gene_exp.squeeze(dim=0)\n",
    "        edgelist = edgelist.squeeze(dim=0).T\n",
    "        celltype = celltype.squeeze(dim=0)\n",
    "\n",
    "        celltype_pred = self.forward(gene_exp, edgelist)\n",
    "\n",
    "        # Calculate metrics\n",
    "        val_overall_acc = self.metric_overall_acc(preds=celltype_pred, target=celltype.reshape(-1))\n",
    "        val_macro_acc = self.metric_macro_acc(preds=celltype_pred, target=celltype.reshape(-1))\n",
    "        val_metric_multiclass_acc = self.metric_multiclass_acc(preds=celltype_pred, target=celltype.reshape(-1))\n",
    "\n",
    "        self.log(\"val_overall_acc\", val_overall_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        self.log(\"val_macro_acc\", val_macro_acc, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        pass\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# data, model and fitting\u001b[39;00m\n\u001b[1;32m     25\u001b[0m datamodule \u001b[38;5;241m=\u001b[39m AnnDataGraphDataModule(data_dir\u001b[38;5;241m=\u001b[39mpaths[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_root\u001b[39m\u001b[38;5;124m\"\u001b[39m], file_names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVISp_nhood.h5ad\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mLitGNNv0\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_genes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_mse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_ce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m trainer \u001b[38;5;241m=\u001b[39m L\u001b[38;5;241m.\u001b[39mTrainer(limit_train_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, limit_val_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, logger\u001b[38;5;241m=\u001b[39mtb_logger, callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback])\n\u001b[1;32m     28\u001b[0m trainer\u001b[38;5;241m.\u001b[39mfit(model\u001b[38;5;241m=\u001b[39mmodel, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m, in \u001b[0;36mLitGNNv0.__init__\u001b[0;34m(self, input_dim, hidden_dim, n_labels, weight_mse, weight_ce)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_mse \u001b[38;5;241m=\u001b[39m weight_mse\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_ce \u001b[38;5;241m=\u001b[39m weight_ce\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mGAT \u001b[38;5;241m=\u001b[39m \u001b[43mGAT3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# losses\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_ce \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m, in \u001b[0;36mGAT3.__init__\u001b[0;34m(self, hidden_channels, num_features, num_classes)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_channels, num_features, num_classes):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1234567\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_channels \u001b[38;5;241m=\u001b[39m hidden_channels\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features \u001b[38;5;241m=\u001b[39m num_features\n",
      "File \u001b[0;32m/data/users1/dkim195/miniconda3/envs/allen/lib/python3.10/site-packages/torch/_compile.py:31\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     disable_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)\n\u001b[1;32m     29\u001b[0m     fn\u001b[38;5;241m.\u001b[39m__dynamo_disable \u001b[38;5;241m=\u001b[39m disable_fn\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdisable_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users1/dkim195/miniconda3/envs/allen/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600\u001b[0m, in \u001b[0;36mDisableContext.__call__.<locals>._fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m prior \u001b[38;5;241m=\u001b[39m set_eval_frame(callback)\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    602\u001b[0m     set_eval_frame(prior)\n",
      "File \u001b[0;32m/data/users1/dkim195/miniconda3/envs/allen/lib/python3.10/site-packages/torch/random.py:46\u001b[0m, in \u001b[0;36mmanual_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mmps\u001b[38;5;241m.\u001b[39m_is_in_bad_fork():\n",
      "File \u001b[0;32m/data/users1/dkim195/miniconda3/envs/allen/lib/python3.10/site-packages/torch/cuda/random.py:127\u001b[0m, in \u001b[0;36mmanual_seed_all\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m    124\u001b[0m         default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[1;32m    125\u001b[0m         default_generator\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m--> 127\u001b[0m \u001b[43m_lazy_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_all\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data/users1/dkim195/miniconda3/envs/allen/lib/python3.10/site-packages/torch/cuda/__init__.py:244\u001b[0m, in \u001b[0;36m_lazy_call\u001b[0;34m(callable, **kwargs)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_lazy_call\u001b[39m(\u001b[38;5;28mcallable\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[0;32m--> 244\u001b[0m         \u001b[38;5;28;43mcallable\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;66;03m# TODO(torch_deploy): this accesses linecache, which attempts to read the\u001b[39;00m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;66;03m# file system to get traceback info. Patch linecache or do something\u001b[39;00m\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# else here if this ends up being important.\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mglobal\u001b[39;00m _lazy_seed_tracker\n",
      "File \u001b[0;32m/data/users1/dkim195/miniconda3/envs/allen/lib/python3.10/site-packages/torch/cuda/random.py:125\u001b[0m, in \u001b[0;36mmanual_seed_all.<locals>.cb\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(device_count()):\n\u001b[1;32m    124\u001b[0m     default_generator \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdefault_generators[i]\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mdefault_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "\n",
    "# data parameters, we'll eventually obtain this from the data.\n",
    "n_genes = 500\n",
    "n_labels = 94\n",
    "\n",
    "# paths\n",
    "paths = get_paths()\n",
    "expname = get_datetime(expname=\"VISp_nhood_GNN\")\n",
    "log_path = paths[\"runs_root\"] + f\"logs/{expname}\"\n",
    "checkpoint_path = paths[\"runs_root\"] + f\"checkpoints/{expname}\"\n",
    "\n",
    "# helpers\n",
    "tb_logger = TensorBoardLogger(save_dir=log_path)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=checkpoint_path, monitor=\"val_rmse_overall\", filename=\"{epoch}-{val_rmse_overall:.2f}\"\n",
    ")\n",
    "\n",
    "# data, model and fitting\n",
    "datamodule = AnnDataGraphDataModule(data_dir=paths[\"data_root\"], file_names=[\"VISp_nhood.h5ad\"], batch_size=1)\n",
    "model = LitGNNv0(input_dim=n_genes, hidden_dim=32, n_labels=n_labels, weight_mse=1.0, weight_ce=0.1)\n",
    "trainer = L.Trainer(\n",
    "    limit_train_batches=1000, limit_val_batches=100, max_epochs=5, logger=tb_logger, callbacks=[checkpoint_callback]\n",
    ")\n",
    "trainer.fit(model=model, datamodule=datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
