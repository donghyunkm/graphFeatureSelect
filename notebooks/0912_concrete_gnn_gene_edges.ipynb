{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edges based on gene expression. Same idea as LR edges. This isn't the best idea because we want to select the genes during training. However, if we find that some gene edges are very informative when used in a GNN, biologists could always select those genes as part of the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import scanpy as sc\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import add_remaining_self_loops\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv\n",
    "import torch.nn as nn\n",
    "from graphFeatureSelect.utils import get_adata\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from torch_geometric.utils import add_remaining_self_loops, from_scipy_sparse_matrix\n",
    "from scipy.sparse import csr_array\n",
    "\n",
    "\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", font_scale=0.5, rc=custom_params)\n",
    "%config InlineBackend.figure_format=\"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gnn_concrete(model, optimizer, data, criterion, temp):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()  # Clear gradients.\n",
    "    out = model(data.x, data.edge_index, temp)  # Perform a single forward pass.\n",
    "    loss = criterion(\n",
    "        out[data.train_mask], data.y[data.train_mask]\n",
    "    )  # Compute the loss solely based on the training nodes.\n",
    "    loss.backward()  # Derive gradients.\n",
    "    optimizer.step()  # Update parameters based on gradients.\n",
    "    return loss\n",
    "\n",
    "\n",
    "def val_gnn_concrete(model, data):\n",
    "    model.eval()\n",
    "    temp = 0.01\n",
    "    out = model(data.x, data.edge_index, temp)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    val_correct = pred[data.val_mask] == data.y[data.val_mask]  # Check against ground-truth labels.\n",
    "    val_acc = int(val_correct.sum()) / int(data.val_mask.sum())  # Derive ratio of correct predictions.\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "def test_gnn_concrete(model, data):\n",
    "    model.eval()\n",
    "    temp = 0.01\n",
    "    out = model(data.x, data.edge_index, temp)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "    test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "    test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATnet_concrete(torch.nn.Module):\n",
    "    def __init__(self, n_mask, hidden_channels, num_features, num_classes):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 = GATv2Conv(num_features, hidden_channels)\n",
    "        self.conv2 = GATv2Conv(hidden_channels, num_classes)\n",
    "        self.n_mask = n_mask\n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "        self.concrete = nn.Parameter(torch.randn(self.n_mask, self.num_features))\n",
    "\n",
    "    def forward(self, x, edge_index, temp):\n",
    "        mask = F.gumbel_softmax(self.concrete, tau=temp, hard=True)\n",
    "        mask = torch.sum(mask, axis=0)\n",
    "        mask = torch.clamp(mask, min=0, max=1)\n",
    "        x = mask * x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "    def softmax(self):\n",
    "        return F.softmax(self.concrete, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users1/dkim195/miniconda3/envs/allen/lib/python3.10/site-packages/anndata/_core/aligned_df.py:68: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "adata = get_adata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brain_section_label</th>\n",
       "      <th>z_section</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C57BL6J-638850.30</th>\n",
       "      <th>5.0</th>\n",
       "      <td>9242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C57BL6J-638850.29</th>\n",
       "      <th>4.8</th>\n",
       "      <td>8713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C57BL6J-638850.28</th>\n",
       "      <th>4.6</th>\n",
       "      <td>7780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>C57BL6J-638850.31</th>\n",
       "      <th>5.4</th>\n",
       "      <td>6939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               count\n",
       "brain_section_label z_section       \n",
       "C57BL6J-638850.30   5.0         9242\n",
       "C57BL6J-638850.29   4.8         8713\n",
       "C57BL6J-638850.28   4.6         7780\n",
       "C57BL6J-638850.31   5.4         6939"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(adata.obs[[\"brain_section_label\", \"z_section\"]].sort_values(\"z_section\").value_counts().to_frame().head(4))\n",
    "one_sec = adata[adata.obs[\"z_section\"] == 5.0, :]\n",
    "df = one_sec.obs.copy()\n",
    "num_nodes = df.shape[0]\n",
    "cell_type = \"supertype\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users1/dkim195/miniconda3/envs/allen/lib/python3.10/site-packages/sklearn/model_selection/_split.py:776: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "train_idx, test_idx = next(skf.split(np.arange(df.shape[0]), df[cell_type].values))\n",
    "\n",
    "train_mask = np.zeros(df.shape[0], dtype=bool)\n",
    "train_mask[train_idx] = True\n",
    "train_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
    "\n",
    "test_mask = np.zeros(df.shape[0], dtype=bool)\n",
    "test_mask[test_idx] = True\n",
    "test_mask = torch.tensor(test_mask, dtype=torch.bool)\n",
    "\n",
    "labels = torch.tensor(df[cell_type].cat.codes.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph construction hyperparameters\n",
    "d = 40 / 1000  # (in mm)\n",
    "L_thr = 0.0\n",
    "R_thr = 0.0\n",
    "lr_gene_pairs = [[\"Tac2\", \"Tacr3\"], [\"Penk\", \"Oprd1\"], [\"Pdyn\", \"Oprd1\"], [\"Pdyn\", \"Oprk1\"], [\"Grp\", \"Grpr\"]]\n",
    "n_layers = len(lr_gene_pairs)\n",
    "num_nodes = df.shape[0]\n",
    "cell_type = \"supertype\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_list = [None] * n_layers\n",
    "df[\"participant\"] = np.zeros(num_nodes, dtype=bool)\n",
    "\n",
    "# Edgelist from multi-layer graphs\n",
    "for i in range(n_layers):\n",
    "    ligand, receptor = lr_gene_pairs[i]\n",
    "    df[\"L\"] = one_sec[:, one_sec.var[\"gene_symbol\"] == ligand].X.toarray().ravel()\n",
    "    df[\"R\"] = one_sec[:, one_sec.var[\"gene_symbol\"] == receptor].X.toarray().ravel()\n",
    "\n",
    "    df[\"L\"] = (df[\"L\"] > L_thr).astype(bool)\n",
    "    df[\"R\"] = (df[\"R\"] > R_thr).astype(bool)\n",
    "\n",
    "    A = df[\"L\"].values.reshape(-1, 1) @ df[\"R\"].values.reshape(1, -1)\n",
    "    Dx = (df[\"x_reconstructed\"].values.reshape(-1, 1) - df[\"x_reconstructed\"].values.reshape(1, -1)) ** 2\n",
    "    Dy = (df[\"y_reconstructed\"].values.reshape(-1, 1) - df[\"y_reconstructed\"].values.reshape(1, -1)) ** 2\n",
    "    D = np.sqrt(Dx + Dy)\n",
    "    del Dx, Dy\n",
    "\n",
    "    # cells are connected only if within distance d\n",
    "    A[D > d] = 0\n",
    "\n",
    "    # participant should have more than one connection\n",
    "    df[\"participant\"] = df[\"participant\"] + (A.sum(axis=1) > 1)\n",
    "\n",
    "    # construct directed graph from adjacency matrix\n",
    "    edge_index_list[i], _ = from_scipy_sparse_matrix(csr_array(A))\n",
    "\n",
    "\n",
    "# Squash the multi-layer graph into a single layer graph\n",
    "edge_index_squashed = set(edge_index_list[0].T)\n",
    "for i in range(1, len(edge_index_list)):\n",
    "    edge_index_squashed = set(edge_index_list[i].T).union(edge_index_squashed)\n",
    "edge_index_squashed = list(edge_index_squashed)\n",
    "edge_index_list.append(edge_index_squashed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_squashed = torch.stack(edge_index_list[-1], dim=0)\n",
    "edgelist_squashed = add_remaining_self_loops(edgelist_squashed.T)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_genes_considered = 100\n",
    "genes_to_select = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_sec_x = torch.tensor(one_sec.X.todense(), dtype=torch.float)\n",
    "one_sec_x_subset = torch.tensor(one_sec.X.todense(), dtype=torch.float)[\n",
    "    :, :num_genes_considered\n",
    "]  # only use first n genes to feature select from\n",
    "\n",
    "data_gene_concrete = Data(\n",
    "    x=one_sec_x_subset, edge_index=edgelist_squashed, y=labels, train_mask=train_mask, test_mask=test_mask\n",
    ")\n",
    "data_gene_concrete_full = Data(\n",
    "    x=one_sec_x, edge_index=edgelist_squashed, y=labels, train_mask=train_mask, test_mask=test_mask\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gene_concrete = GATnet_concrete(\n",
    "    n_mask=genes_to_select,\n",
    "    hidden_channels=32,\n",
    "    num_features=data_gene_concrete.x.shape[1],\n",
    "    num_classes=torch.unique(data_gene_concrete.y).size()[0],\n",
    ")\n",
    "model_gene_concrete_full = GATnet_concrete(\n",
    "    n_mask=genes_to_select,\n",
    "    hidden_channels=32,\n",
    "    num_features=data_gene_concrete_full.x.shape[1],\n",
    "    num_classes=torch.unique(data_gene_concrete_full.y).size()[0],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_temp_schedule(epoch):\n",
    "    return 10 * (1 - epoch / 1000) + 1e-3\n",
    "\n",
    "\n",
    "def exp_decay_temp_schedule(epoch, total_epoch):\n",
    "    start_temp = 10\n",
    "    end_temp = 0.01\n",
    "    temp = start_temp * (end_temp / start_temp) ** (epoch / total_epoch)\n",
    "    return temp\n",
    "\n",
    "\n",
    "# Article from Ian Covert says using temp = 0.1 throughout training is effective as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200, Loss: 3.2590, Val acc: 0.1920\n",
      "Epoch: 400, Loss: 3.1533, Val acc: 0.1449\n",
      "Epoch: 600, Loss: 3.1360, Val acc: 0.3407\n",
      "Epoch: 800, Loss: 2.9140, Val acc: 0.3121\n",
      "Epoch: 1000, Loss: 3.0935, Val acc: 0.2731\n",
      "Gene edge accuracy: 0.2704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.2704164413196322"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model_gene_concrete to predict cell types\n",
    "def gene_edges_concrete(model_gene, data_gene):\n",
    "    optimizer_gene = torch.optim.Adam(model_gene.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, 1001):\n",
    "        loss = train_gnn_concrete(model_gene, optimizer_gene, data_gene, criterion, 0.1)\n",
    "        val_acc = test_gnn_concrete(model_gene, data_gene)\n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    model_gene.eval()\n",
    "    val_acc = test_gnn_concrete(model_gene, data_gene)\n",
    "    print(f\"Gene edge accuracy: {val_acc:.4f}\")\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "gene_edges_concrete(model_gene_concrete, data_gene_concrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0570, grad_fn=<MaxBackward1>)\n",
      "tensor(47)\n",
      "tensor(0.1102, grad_fn=<MaxBackward1>)\n",
      "tensor(40)\n",
      "tensor(0.0660, grad_fn=<MaxBackward1>)\n",
      "tensor(39)\n",
      "tensor(0.1195, grad_fn=<MaxBackward1>)\n",
      "tensor(78)\n",
      "tensor(0.0577, grad_fn=<MaxBackward1>)\n",
      "tensor(67)\n",
      "tensor(0.0572, grad_fn=<MaxBackward1>)\n",
      "tensor(48)\n",
      "tensor(0.0459, grad_fn=<MaxBackward1>)\n",
      "tensor(32)\n",
      "tensor(0.0707, grad_fn=<MaxBackward1>)\n",
      "tensor(75)\n",
      "tensor(0.0580, grad_fn=<MaxBackward1>)\n",
      "tensor(73)\n",
      "tensor(0.0567, grad_fn=<MaxBackward1>)\n",
      "tensor(74)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_374225/461750568.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(torch.max(F.softmax(model_gene_concrete.concrete[i])))\n",
      "/tmp/ipykernel_374225/461750568.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(torch.argmax(F.softmax(model_gene_concrete.concrete[i])))\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model_gene_concrete.concrete)):\n",
    "    print(torch.max(F.softmax(model_gene_concrete.concrete[i])))\n",
    "    print(torch.argmax(F.softmax(model_gene_concrete.concrete[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 200, Loss: 3.2190, Val acc: 0.1833\n",
      "Epoch: 400, Loss: 3.1513, Val acc: 0.1466\n",
      "Epoch: 600, Loss: 3.1007, Val acc: 0.2445\n",
      "Epoch: 800, Loss: 3.0798, Val acc: 0.1747\n",
      "Epoch: 1000, Loss: 2.9429, Val acc: 0.1520\n",
      "Gene edge accuracy: 0.2985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.298539751216874"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model_gene_concrete_full to predict cell types\n",
    "def gene_edges_concrete_full(model_gene_full, data_gene):\n",
    "    optimizer_gene = torch.optim.Adam(model_gene_full.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1, 2001):\n",
    "        loss = train_gnn_concrete(model_gene_full, optimizer_gene, data_gene, criterion, 0.1)\n",
    "        val_acc = test_gnn_concrete(model_gene_full, data_gene)\n",
    "        if epoch % 200 == 0:\n",
    "            print(f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    model_gene_full.eval()\n",
    "    val_acc = test_gnn_concrete(model_gene_full, data_gene)\n",
    "    print(f\"Gene edge accuracy: {val_acc:.4f}\")\n",
    "    return val_acc\n",
    "\n",
    "\n",
    "gene_edges_concrete(model_gene_concrete_full, data_gene_concrete_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0117, grad_fn=<MaxBackward1>)\n",
      "tensor(83)\n",
      "tensor(0.0156, grad_fn=<MaxBackward1>)\n",
      "tensor(266)\n",
      "tensor(0.0313, grad_fn=<MaxBackward1>)\n",
      "tensor(326)\n",
      "tensor(0.0137, grad_fn=<MaxBackward1>)\n",
      "tensor(21)\n",
      "tensor(0.0244, grad_fn=<MaxBackward1>)\n",
      "tensor(28)\n",
      "tensor(0.0207, grad_fn=<MaxBackward1>)\n",
      "tensor(495)\n",
      "tensor(0.0057, grad_fn=<MaxBackward1>)\n",
      "tensor(178)\n",
      "tensor(0.0072, grad_fn=<MaxBackward1>)\n",
      "tensor(56)\n",
      "tensor(0.0095, grad_fn=<MaxBackward1>)\n",
      "tensor(517)\n",
      "tensor(0.0125, grad_fn=<MaxBackward1>)\n",
      "tensor(300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_366472/523707653.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(torch.max(F.softmax(model_self_concrete_full.concrete[i])))\n",
      "/tmp/ipykernel_366472/523707653.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  print(torch.argmax(F.softmax(model_self_concrete_full.concrete[i])))\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(model_gene_concrete_full.concrete)):\n",
    "    print(torch.max(F.softmax(model_gene_concrete_full.concrete[i])))\n",
    "    print(torch.argmax(F.softmax(model_gene_concrete_full.concrete[i])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
