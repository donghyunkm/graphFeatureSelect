{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we explore feature importance modules provided by captum using toy datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", font_scale=0.8, rc=custom_params)\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "x, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=3,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=4,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=None,\n",
    "    flip_y=0.01,\n",
    "    class_sep=1.0,\n",
    "    hypercube=True,\n",
    "    shift=0.0,\n",
    "    scale=1.0,\n",
    "    shuffle=False,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x with only informative features\n",
    "# x = x[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [f\"x{i}\" for i in range(1, x.shape[1] + 1)]\n",
    "df = pd.DataFrame(x, columns=feature_names)\n",
    "df[\"y\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 2000\n",
    "learning_rate = 0.01\n",
    "size_hidden1 = 100\n",
    "size_hidden2 = 50\n",
    "size_hidden3 = 10\n",
    "size_hidden4 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Keep cells who are participants in the multilayer graph (more than 1 connection)\n",
    "train_idx, test_idx = next(skf.split(np.arange(x.shape[0]), y))\n",
    "\n",
    "train_mask = np.zeros(x.shape[0], dtype=bool)\n",
    "train_mask[train_idx] = True\n",
    "train_mask = torch.tensor(train_mask, dtype=torch.bool)\n",
    "\n",
    "test_mask = np.zeros(x.shape[0], dtype=bool)\n",
    "test_mask[test_idx] = True\n",
    "test_mask = torch.tensor(test_mask, dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist_self = torch.tensor([[i, i] for i in range(x.shape[0])])\n",
    "edgelist_self = edgelist_self.T\n",
    "\n",
    "one_sec_x = torch.tensor(x, dtype=torch.float)\n",
    "labels = torch.tensor(y, dtype=torch.long)\n",
    "data_self = Data(x=one_sec_x, edge_index=edgelist_self, y=labels, train_mask=train_mask, test_mask=test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay_temp_schedule(epoch, total_epoch):\n",
    "    start_temp = 10\n",
    "    end_temp = 0.01\n",
    "    temp = start_temp * (end_temp / start_temp) ** (epoch / total_epoch)\n",
    "    return temp\n",
    "\n",
    "\n",
    "def eval_stats_concrete(model_inp, data):\n",
    "    model_inp.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_inp(data.x[data.test_mask], data.edge_index, 0.01, True)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total = data.y[data.test_mask].size(0)\n",
    "        correct = (predicted == data.y[data.test_mask].view(-1)).sum().item()\n",
    "        print(\"Accuracy of the network on the test set: %d %%\" % (100 * correct / total))\n",
    "\n",
    "\n",
    "def trainConcrete(model_inp, data, num_epochs=num_epochs):\n",
    "    optimizer = torch.optim.Adam(model_inp.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        outputs = model_inp(data.x, data.edge_index, exp_decay_temp_schedule(epoch, num_epochs), False)\n",
    "        loss = criterion(outputs[data.train_mask], data.y[data.train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        running_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            eval_stats_concrete(model_inp, data)\n",
    "            print(\n",
    "                \"Epoch [%d]/[%d] running accumulative loss across all batches: %.3f\"\n",
    "                % (epoch + 1, num_epochs, running_loss)\n",
    "            )\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConcrete(\n",
       "  (conv1): GATv2Conv(10, 6, heads=8)\n",
       "  (conv2): GATv2Conv(6, 4, heads=8)\n",
       "  (lin1): Linear(in_features=10, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch_geometric.nn import GATv2Conv\n",
    "\n",
    "\n",
    "class ModelConcrete(torch.nn.Module):\n",
    "    def __init__(self, n_mask, hidden_channels, num_features=10, n_classes=4):\n",
    "        super().__init__()\n",
    "        self.n_mask = n_mask\n",
    "        self.num_classes = n_classes\n",
    "        self.num_features = num_features\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.concrete = nn.Parameter(torch.randn(self.n_mask, self.num_features))\n",
    "\n",
    "        self.conv1 = GATv2Conv(self.num_features, self.hidden_channels, heads=8, concat=False)\n",
    "        self.conv2 = GATv2Conv(self.hidden_channels, self.num_classes, heads=8, concat=False)\n",
    "        self.lin1 = nn.Linear(num_features, self.num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "    def forward(self, x, edge_index, temp, hard_):\n",
    "        mask = F.gumbel_softmax(self.concrete, tau=temp, hard=hard_)\n",
    "        mask = torch.sum(mask, axis=0)\n",
    "        mask = torch.clamp(mask, min=0, max=1)\n",
    "        x = mask * x\n",
    "\n",
    "        residual1 = self.lin1(x)\n",
    "\n",
    "        out = self.conv1(x, edge_index)\n",
    "        out = out.relu()\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        out = self.conv2(out, edge_index)\n",
    "        out = out + residual1\n",
    "\n",
    "        return out\n",
    "\n",
    "    def softmax(self):\n",
    "        return F.softmax(self.concrete, dim=1)\n",
    "\n",
    "\n",
    "modelConcrete = ModelConcrete(n_mask=3, hidden_channels=6, num_features=x.shape[1], n_classes=np.unique(y).size)\n",
    "modelConcrete.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider adding LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 18 %\n",
      "Epoch [1]/[2000] running accumulative loss across all batches: 1.409\n",
      "Accuracy of the network on the test set: 59 %\n",
      "Epoch [21]/[2000] running accumulative loss across all batches: 1.101\n",
      "Accuracy of the network on the test set: 47 %\n",
      "Epoch [41]/[2000] running accumulative loss across all batches: 0.794\n",
      "Accuracy of the network on the test set: 36 %\n",
      "Epoch [61]/[2000] running accumulative loss across all batches: 0.586\n",
      "Accuracy of the network on the test set: 63 %\n",
      "Epoch [81]/[2000] running accumulative loss across all batches: 0.481\n",
      "Accuracy of the network on the test set: 40 %\n",
      "Epoch [101]/[2000] running accumulative loss across all batches: 0.419\n",
      "Accuracy of the network on the test set: 84 %\n",
      "Epoch [121]/[2000] running accumulative loss across all batches: 0.364\n",
      "Accuracy of the network on the test set: 66 %\n",
      "Epoch [141]/[2000] running accumulative loss across all batches: 0.336\n",
      "Accuracy of the network on the test set: 83 %\n",
      "Epoch [161]/[2000] running accumulative loss across all batches: 0.318\n",
      "Accuracy of the network on the test set: 47 %\n",
      "Epoch [181]/[2000] running accumulative loss across all batches: 0.301\n",
      "Accuracy of the network on the test set: 63 %\n",
      "Epoch [201]/[2000] running accumulative loss across all batches: 0.360\n",
      "Accuracy of the network on the test set: 39 %\n",
      "Epoch [221]/[2000] running accumulative loss across all batches: 0.317\n",
      "Accuracy of the network on the test set: 63 %\n",
      "Epoch [241]/[2000] running accumulative loss across all batches: 0.284\n",
      "Accuracy of the network on the test set: 62 %\n",
      "Epoch [261]/[2000] running accumulative loss across all batches: 0.283\n",
      "Accuracy of the network on the test set: 65 %\n",
      "Epoch [281]/[2000] running accumulative loss across all batches: 0.275\n",
      "Accuracy of the network on the test set: 63 %\n",
      "Epoch [301]/[2000] running accumulative loss across all batches: 0.267\n",
      "Accuracy of the network on the test set: 65 %\n",
      "Epoch [321]/[2000] running accumulative loss across all batches: 0.329\n",
      "Accuracy of the network on the test set: 66 %\n",
      "Epoch [341]/[2000] running accumulative loss across all batches: 0.273\n",
      "Accuracy of the network on the test set: 85 %\n",
      "Epoch [361]/[2000] running accumulative loss across all batches: 0.366\n",
      "Accuracy of the network on the test set: 69 %\n",
      "Epoch [381]/[2000] running accumulative loss across all batches: 0.267\n",
      "Accuracy of the network on the test set: 63 %\n",
      "Epoch [401]/[2000] running accumulative loss across all batches: 0.407\n",
      "Accuracy of the network on the test set: 63 %\n",
      "Epoch [421]/[2000] running accumulative loss across all batches: 0.384\n",
      "Accuracy of the network on the test set: 65 %\n",
      "Epoch [441]/[2000] running accumulative loss across all batches: 0.322\n",
      "Accuracy of the network on the test set: 62 %\n",
      "Epoch [461]/[2000] running accumulative loss across all batches: 0.264\n",
      "Accuracy of the network on the test set: 69 %\n",
      "Epoch [481]/[2000] running accumulative loss across all batches: 0.293\n",
      "Accuracy of the network on the test set: 66 %\n",
      "Epoch [501]/[2000] running accumulative loss across all batches: 0.331\n",
      "Accuracy of the network on the test set: 64 %\n",
      "Epoch [521]/[2000] running accumulative loss across all batches: 0.409\n",
      "Accuracy of the network on the test set: 64 %\n",
      "Epoch [541]/[2000] running accumulative loss across all batches: 0.334\n",
      "Accuracy of the network on the test set: 40 %\n",
      "Epoch [561]/[2000] running accumulative loss across all batches: 0.268\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [581]/[2000] running accumulative loss across all batches: 0.296\n",
      "Accuracy of the network on the test set: 64 %\n",
      "Epoch [601]/[2000] running accumulative loss across all batches: 0.301\n",
      "Accuracy of the network on the test set: 60 %\n",
      "Epoch [621]/[2000] running accumulative loss across all batches: 0.333\n",
      "Accuracy of the network on the test set: 68 %\n",
      "Epoch [641]/[2000] running accumulative loss across all batches: 0.357\n",
      "Accuracy of the network on the test set: 71 %\n",
      "Epoch [661]/[2000] running accumulative loss across all batches: 0.316\n",
      "Accuracy of the network on the test set: 64 %\n",
      "Epoch [681]/[2000] running accumulative loss across all batches: 0.413\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [701]/[2000] running accumulative loss across all batches: 0.338\n",
      "Accuracy of the network on the test set: 67 %\n",
      "Epoch [721]/[2000] running accumulative loss across all batches: 0.404\n",
      "Accuracy of the network on the test set: 66 %\n",
      "Epoch [741]/[2000] running accumulative loss across all batches: 0.697\n",
      "Accuracy of the network on the test set: 67 %\n",
      "Epoch [761]/[2000] running accumulative loss across all batches: 0.634\n",
      "Accuracy of the network on the test set: 68 %\n",
      "Epoch [781]/[2000] running accumulative loss across all batches: 0.317\n",
      "Accuracy of the network on the test set: 64 %\n",
      "Epoch [801]/[2000] running accumulative loss across all batches: 0.349\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [821]/[2000] running accumulative loss across all batches: 0.261\n",
      "Accuracy of the network on the test set: 66 %\n",
      "Epoch [841]/[2000] running accumulative loss across all batches: 0.271\n",
      "Accuracy of the network on the test set: 67 %\n",
      "Epoch [861]/[2000] running accumulative loss across all batches: 0.250\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [881]/[2000] running accumulative loss across all batches: 0.252\n",
      "Accuracy of the network on the test set: 60 %\n",
      "Epoch [901]/[2000] running accumulative loss across all batches: 0.250\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [921]/[2000] running accumulative loss across all batches: 0.526\n",
      "Accuracy of the network on the test set: 65 %\n",
      "Epoch [941]/[2000] running accumulative loss across all batches: 0.256\n",
      "Accuracy of the network on the test set: 87 %\n",
      "Epoch [961]/[2000] running accumulative loss across all batches: 0.253\n",
      "Accuracy of the network on the test set: 66 %\n",
      "Epoch [981]/[2000] running accumulative loss across all batches: 0.261\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1001]/[2000] running accumulative loss across all batches: 0.594\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1021]/[2000] running accumulative loss across all batches: 0.662\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1041]/[2000] running accumulative loss across all batches: 0.243\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1061]/[2000] running accumulative loss across all batches: 0.489\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1081]/[2000] running accumulative loss across all batches: 0.244\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1101]/[2000] running accumulative loss across all batches: 0.245\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1121]/[2000] running accumulative loss across all batches: 0.256\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1141]/[2000] running accumulative loss across all batches: 0.238\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1161]/[2000] running accumulative loss across all batches: 0.302\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1181]/[2000] running accumulative loss across all batches: 0.236\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1201]/[2000] running accumulative loss across all batches: 0.233\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1221]/[2000] running accumulative loss across all batches: 0.230\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1241]/[2000] running accumulative loss across all batches: 0.231\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1261]/[2000] running accumulative loss across all batches: 0.233\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1281]/[2000] running accumulative loss across all batches: 0.230\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1301]/[2000] running accumulative loss across all batches: 0.228\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1321]/[2000] running accumulative loss across all batches: 0.227\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1341]/[2000] running accumulative loss across all batches: 0.226\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1361]/[2000] running accumulative loss across all batches: 0.228\n",
      "Accuracy of the network on the test set: 66 %\n",
      "Epoch [1381]/[2000] running accumulative loss across all batches: 0.234\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1401]/[2000] running accumulative loss across all batches: 0.227\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1421]/[2000] running accumulative loss across all batches: 0.227\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1441]/[2000] running accumulative loss across all batches: 0.716\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1461]/[2000] running accumulative loss across all batches: 0.228\n",
      "Accuracy of the network on the test set: 69 %\n",
      "Epoch [1481]/[2000] running accumulative loss across all batches: 0.226\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1501]/[2000] running accumulative loss across all batches: 0.226\n",
      "Accuracy of the network on the test set: 62 %\n",
      "Epoch [1521]/[2000] running accumulative loss across all batches: 0.225\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1541]/[2000] running accumulative loss across all batches: 0.225\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1561]/[2000] running accumulative loss across all batches: 0.223\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1581]/[2000] running accumulative loss across all batches: 0.223\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1601]/[2000] running accumulative loss across all batches: 0.222\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1621]/[2000] running accumulative loss across all batches: 0.225\n",
      "Accuracy of the network on the test set: 65 %\n",
      "Epoch [1641]/[2000] running accumulative loss across all batches: 0.770\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1661]/[2000] running accumulative loss across all batches: 0.222\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1681]/[2000] running accumulative loss across all batches: 0.222\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1701]/[2000] running accumulative loss across all batches: 0.845\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1721]/[2000] running accumulative loss across all batches: 0.225\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1741]/[2000] running accumulative loss across all batches: 0.222\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1761]/[2000] running accumulative loss across all batches: 0.222\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1781]/[2000] running accumulative loss across all batches: 0.217\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1801]/[2000] running accumulative loss across all batches: 0.215\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1821]/[2000] running accumulative loss across all batches: 0.215\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1841]/[2000] running accumulative loss across all batches: 0.219\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1861]/[2000] running accumulative loss across all batches: 0.214\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1881]/[2000] running accumulative loss across all batches: 0.213\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1901]/[2000] running accumulative loss across all batches: 0.219\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1921]/[2000] running accumulative loss across all batches: 0.221\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1941]/[2000] running accumulative loss across all batches: 0.215\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1961]/[2000] running accumulative loss across all batches: 0.216\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1981]/[2000] running accumulative loss across all batches: 0.214\n"
     ]
    }
   ],
   "source": [
    "trainConcrete(modelConcrete, data_self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<ClampBackward1>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = F.gumbel_softmax(modelConcrete.concrete, tau=0.01, hard=True)\n",
    "mask = torch.sum(mask, axis=0)\n",
    "mask = torch.clamp(mask, min=0, max=1)\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
