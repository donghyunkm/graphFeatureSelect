{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we explore feature importance modules provided by captum using toy datasets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn.functional as F\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "custom_params = {\"axes.spines.right\": False, \"axes.spines.top\": False}\n",
    "sns.set_theme(style=\"ticks\", font_scale=0.8, rc=custom_params)\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "x, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=3,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=4,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=None,\n",
    "    flip_y=0.01,\n",
    "    class_sep=1.0,\n",
    "    hypercube=True,\n",
    "    shift=0.0,\n",
    "    scale=1.0,\n",
    "    shuffle=False,\n",
    "    random_state=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x with only informative features\n",
    "# x = x[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [f\"x{i}\" for i in range(1, x.shape[1] + 1)]\n",
    "df = pd.DataFrame(x, columns=feature_names)\n",
    "df[\"y\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_epochs = 2000\n",
    "learning_rate = 0.001\n",
    "size_hidden1 = 100\n",
    "size_hidden2 = 50\n",
    "size_hidden3 = 10\n",
    "size_hidden4 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train).float()\n",
    "y_train = torch.tensor(y_train).view(-1, 1).long()\n",
    "\n",
    "x_test = torch.tensor(x_test).float()\n",
    "y_test = torch.tensor(y_test).view(-1, 1).long()\n",
    "\n",
    "datasets = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(datasets, batch_size=50, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (lin1): Linear(in_features=10, out_features=100, bias=True)\n",
       "  (lin2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (lin3): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (lin4): Linear(in_features=10, out_features=4, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size=10, n_classes=4):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(input_size, size_hidden1)\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n",
    "        self.lin4 = nn.Linear(size_hidden3, n_classes)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.gelu(self.lin1(input))\n",
    "        x = self.gelu(self.lin2(x))\n",
    "        x = self.gelu(self.lin3(x))\n",
    "        logits = self.gelu(self.lin4(x))\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = Model(input_size=x.shape[1], n_classes=np.unique(y).size)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_stats(model_inp, x_test, y_test):\n",
    "    model_inp.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_inp(x_test)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total = y_test.size(0)\n",
    "        correct = (predicted == y_test.view(-1)).sum().item()\n",
    "        print(\"Accuracy of the network on the test set: %d %%\" % (100 * correct / total))\n",
    "\n",
    "\n",
    "def train(model_inp, num_epochs=num_epochs):\n",
    "    optimizer = torch.optim.Adam(model_inp.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_iter:\n",
    "            outputs = model_inp(inputs)\n",
    "            loss = criterion(outputs, labels.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            eval_stats(model_inp, x_test, y_test)\n",
    "            print(\n",
    "                \"Epoch [%d]/[%d] running accumulative loss across all batches: %.3f\"\n",
    "                % (epoch + 1, num_epochs, running_loss)\n",
    "            )\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 30 %\n",
      "Epoch [1]/[2000] running accumulative loss across all batches: 19.445\n",
      "Accuracy of the network on the test set: 72 %\n",
      "Epoch [21]/[2000] running accumulative loss across all batches: 16.205\n",
      "Accuracy of the network on the test set: 83 %\n",
      "Epoch [41]/[2000] running accumulative loss across all batches: 7.993\n",
      "Accuracy of the network on the test set: 84 %\n",
      "Epoch [61]/[2000] running accumulative loss across all batches: 5.368\n",
      "Accuracy of the network on the test set: 85 %\n",
      "Epoch [81]/[2000] running accumulative loss across all batches: 4.386\n",
      "Accuracy of the network on the test set: 86 %\n",
      "Epoch [101]/[2000] running accumulative loss across all batches: 3.845\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [121]/[2000] running accumulative loss across all batches: 3.475\n",
      "Accuracy of the network on the test set: 87 %\n",
      "Epoch [141]/[2000] running accumulative loss across all batches: 3.194\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [161]/[2000] running accumulative loss across all batches: 2.975\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [181]/[2000] running accumulative loss across all batches: 2.803\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [201]/[2000] running accumulative loss across all batches: 2.652\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [221]/[2000] running accumulative loss across all batches: 2.535\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [241]/[2000] running accumulative loss across all batches: 2.417\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [261]/[2000] running accumulative loss across all batches: 2.317\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [281]/[2000] running accumulative loss across all batches: 2.232\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [301]/[2000] running accumulative loss across all batches: 2.143\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [321]/[2000] running accumulative loss across all batches: 2.066\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [341]/[2000] running accumulative loss across all batches: 1.991\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [361]/[2000] running accumulative loss across all batches: 1.927\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [381]/[2000] running accumulative loss across all batches: 1.852\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [401]/[2000] running accumulative loss across all batches: 1.782\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [421]/[2000] running accumulative loss across all batches: 1.711\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [441]/[2000] running accumulative loss across all batches: 1.648\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [461]/[2000] running accumulative loss across all batches: 1.581\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [481]/[2000] running accumulative loss across all batches: 1.523\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [501]/[2000] running accumulative loss across all batches: 1.457\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [521]/[2000] running accumulative loss across all batches: 1.400\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [541]/[2000] running accumulative loss across all batches: 1.345\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [561]/[2000] running accumulative loss across all batches: 1.290\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [581]/[2000] running accumulative loss across all batches: 1.240\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [601]/[2000] running accumulative loss across all batches: 1.193\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [621]/[2000] running accumulative loss across all batches: 1.146\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [641]/[2000] running accumulative loss across all batches: 1.096\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [661]/[2000] running accumulative loss across all batches: 1.046\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [681]/[2000] running accumulative loss across all batches: 0.995\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [701]/[2000] running accumulative loss across all batches: 0.943\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [721]/[2000] running accumulative loss across all batches: 0.896\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [741]/[2000] running accumulative loss across all batches: 0.854\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [761]/[2000] running accumulative loss across all batches: 0.808\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [781]/[2000] running accumulative loss across all batches: 0.769\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [801]/[2000] running accumulative loss across all batches: 0.728\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [821]/[2000] running accumulative loss across all batches: 0.686\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [841]/[2000] running accumulative loss across all batches: 0.649\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [861]/[2000] running accumulative loss across all batches: 0.610\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [881]/[2000] running accumulative loss across all batches: 0.569\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [901]/[2000] running accumulative loss across all batches: 0.533\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [921]/[2000] running accumulative loss across all batches: 0.497\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [941]/[2000] running accumulative loss across all batches: 0.465\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [961]/[2000] running accumulative loss across all batches: 0.420\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [981]/[2000] running accumulative loss across all batches: 0.387\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1001]/[2000] running accumulative loss across all batches: 0.361\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1021]/[2000] running accumulative loss across all batches: 0.335\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1041]/[2000] running accumulative loss across all batches: 0.314\n",
      "Accuracy of the network on the test set: 87 %\n",
      "Epoch [1061]/[2000] running accumulative loss across all batches: 0.295\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1081]/[2000] running accumulative loss across all batches: 0.278\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1101]/[2000] running accumulative loss across all batches: 0.263\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1121]/[2000] running accumulative loss across all batches: 0.250\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1141]/[2000] running accumulative loss across all batches: 0.238\n",
      "Accuracy of the network on the test set: 87 %\n",
      "Epoch [1161]/[2000] running accumulative loss across all batches: 0.228\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1181]/[2000] running accumulative loss across all batches: 0.218\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1201]/[2000] running accumulative loss across all batches: 0.210\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1221]/[2000] running accumulative loss across all batches: 0.203\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1241]/[2000] running accumulative loss across all batches: 0.196\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1261]/[2000] running accumulative loss across all batches: 0.191\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1281]/[2000] running accumulative loss across all batches: 0.186\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1301]/[2000] running accumulative loss across all batches: 0.182\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1321]/[2000] running accumulative loss across all batches: 0.178\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1341]/[2000] running accumulative loss across all batches: 0.175\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1361]/[2000] running accumulative loss across all batches: 0.172\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1381]/[2000] running accumulative loss across all batches: 0.170\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1401]/[2000] running accumulative loss across all batches: 0.168\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1421]/[2000] running accumulative loss across all batches: 0.167\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1441]/[2000] running accumulative loss across all batches: 0.166\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1461]/[2000] running accumulative loss across all batches: 0.164\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1481]/[2000] running accumulative loss across all batches: 0.163\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1501]/[2000] running accumulative loss across all batches: 0.162\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1521]/[2000] running accumulative loss across all batches: 0.162\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1541]/[2000] running accumulative loss across all batches: 0.161\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1561]/[2000] running accumulative loss across all batches: 0.161\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1581]/[2000] running accumulative loss across all batches: 0.160\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1601]/[2000] running accumulative loss across all batches: 0.160\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1621]/[2000] running accumulative loss across all batches: 0.160\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1641]/[2000] running accumulative loss across all batches: 0.160\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1661]/[2000] running accumulative loss across all batches: 0.159\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1681]/[2000] running accumulative loss across all batches: 0.159\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1701]/[2000] running accumulative loss across all batches: 0.160\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1721]/[2000] running accumulative loss across all batches: 0.159\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1741]/[2000] running accumulative loss across all batches: 0.158\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1761]/[2000] running accumulative loss across all batches: 0.159\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1781]/[2000] running accumulative loss across all batches: 0.158\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1801]/[2000] running accumulative loss across all batches: 0.159\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1821]/[2000] running accumulative loss across all batches: 0.159\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1841]/[2000] running accumulative loss across all batches: 0.158\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1861]/[2000] running accumulative loss across all batches: 0.159\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1881]/[2000] running accumulative loss across all batches: 0.158\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1901]/[2000] running accumulative loss across all batches: 0.158\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1921]/[2000] running accumulative loss across all batches: 0.158\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1941]/[2000] running accumulative loss across all batches: 0.158\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1961]/[2000] running accumulative loss across all batches: 0.159\n",
      "Accuracy of the network on the test set: 88 %\n",
      "Epoch [1981]/[2000] running accumulative loss across all batches: 0.158\n"
     ]
    }
   ],
   "source": [
    "train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelConcrete(\n",
       "  (lin1): Linear(in_features=10, out_features=100, bias=True)\n",
       "  (lin2): Linear(in_features=100, out_features=50, bias=True)\n",
       "  (lin3): Linear(in_features=50, out_features=10, bias=True)\n",
       "  (lin4): Linear(in_features=10, out_features=4, bias=True)\n",
       "  (gelu): GELU(approximate='none')\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelConcrete(torch.nn.Module):\n",
    "    def __init__(self, n_mask, input_size=10, n_classes=4):\n",
    "        super().__init__()\n",
    "        self.n_mask = n_mask\n",
    "        self.num_features = input_size\n",
    "        self.num_classes = n_classes\n",
    "        self.concrete = nn.Parameter(torch.randn(self.n_mask, self.num_features))\n",
    "        self.lin1 = nn.Linear(input_size, size_hidden1)\n",
    "        self.lin2 = nn.Linear(size_hidden1, size_hidden2)\n",
    "        self.lin3 = nn.Linear(size_hidden2, size_hidden3)\n",
    "        self.lin4 = nn.Linear(size_hidden3, n_classes)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x, temp, hard_):\n",
    "        mask = F.gumbel_softmax(self.concrete, tau=temp, hard=hard_)\n",
    "        mask = torch.sum(mask, axis=0)\n",
    "        mask = torch.clamp(mask, min=0, max=1)\n",
    "        x = mask * x\n",
    "        x = self.gelu(self.lin1(x))\n",
    "        x = self.gelu(self.lin2(x))\n",
    "        x = self.gelu(self.lin3(x))\n",
    "        logits = self.gelu(self.lin4(x))\n",
    "        return logits\n",
    "\n",
    "    def softmax(self):\n",
    "        return F.softmax(self.concrete, dim=1)\n",
    "\n",
    "\n",
    "modelConcrete = ModelConcrete(n_mask=3, input_size=x.shape[1], n_classes=np.unique(y).size)\n",
    "modelConcrete.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_decay_temp_schedule(epoch, total_epoch):\n",
    "    start_temp = 10\n",
    "    end_temp = 0.01\n",
    "    temp = start_temp * (end_temp / start_temp) ** (epoch / total_epoch)\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider adding LR scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_statsConcrete(model_inp, x_test, y_test):\n",
    "    model_inp.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model_inp(x_test, 0.01, True)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total = y_test.size(0)\n",
    "        correct = (predicted == y_test.view(-1)).sum().item()\n",
    "        print(\"Accuracy of the network on the test set: %d %%\" % (100 * correct / total))\n",
    "\n",
    "\n",
    "def trainConcrete(model_inp, num_epochs=num_epochs):\n",
    "    optimizer = torch.optim.Adam(model_inp.parameters(), lr=learning_rate)\n",
    "    for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_iter:\n",
    "            outputs = model_inp(inputs, exp_decay_temp_schedule(epoch, num_epochs), False)\n",
    "            loss = criterion(outputs, labels.view(-1))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            eval_statsConcrete(model_inp, x_test, y_test)\n",
    "            print(\n",
    "                \"Epoch [%d]/[%d] running accumulative loss across all batches: %.3f\"\n",
    "                % (epoch + 1, num_epochs, running_loss)\n",
    "            )\n",
    "        running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test set: 20 %\n",
      "Epoch [1]/[2000] running accumulative loss across all batches: 19.385\n",
      "Accuracy of the network on the test set: 42 %\n",
      "Epoch [21]/[2000] running accumulative loss across all batches: 4.039\n",
      "Accuracy of the network on the test set: 37 %\n",
      "Epoch [41]/[2000] running accumulative loss across all batches: 2.945\n",
      "Accuracy of the network on the test set: 24 %\n",
      "Epoch [61]/[2000] running accumulative loss across all batches: 2.635\n",
      "Accuracy of the network on the test set: 46 %\n",
      "Epoch [81]/[2000] running accumulative loss across all batches: 2.315\n",
      "Accuracy of the network on the test set: 36 %\n",
      "Epoch [101]/[2000] running accumulative loss across all batches: 2.104\n",
      "Accuracy of the network on the test set: 24 %\n",
      "Epoch [121]/[2000] running accumulative loss across all batches: 2.086\n",
      "Accuracy of the network on the test set: 38 %\n",
      "Epoch [141]/[2000] running accumulative loss across all batches: 1.756\n",
      "Accuracy of the network on the test set: 24 %\n",
      "Epoch [161]/[2000] running accumulative loss across all batches: 1.705\n",
      "Accuracy of the network on the test set: 69 %\n",
      "Epoch [181]/[2000] running accumulative loss across all batches: 1.613\n",
      "Accuracy of the network on the test set: 35 %\n",
      "Epoch [201]/[2000] running accumulative loss across all batches: 1.410\n",
      "Accuracy of the network on the test set: 38 %\n",
      "Epoch [221]/[2000] running accumulative loss across all batches: 1.555\n",
      "Accuracy of the network on the test set: 62 %\n",
      "Epoch [241]/[2000] running accumulative loss across all batches: 1.716\n",
      "Accuracy of the network on the test set: 65 %\n",
      "Epoch [261]/[2000] running accumulative loss across all batches: 2.269\n",
      "Accuracy of the network on the test set: 47 %\n",
      "Epoch [281]/[2000] running accumulative loss across all batches: 1.261\n",
      "Accuracy of the network on the test set: 36 %\n",
      "Epoch [301]/[2000] running accumulative loss across all batches: 1.314\n",
      "Accuracy of the network on the test set: 62 %\n",
      "Epoch [321]/[2000] running accumulative loss across all batches: 1.563\n",
      "Accuracy of the network on the test set: 33 %\n",
      "Epoch [341]/[2000] running accumulative loss across all batches: 2.347\n",
      "Accuracy of the network on the test set: 40 %\n",
      "Epoch [361]/[2000] running accumulative loss across all batches: 2.768\n",
      "Accuracy of the network on the test set: 25 %\n",
      "Epoch [381]/[2000] running accumulative loss across all batches: 1.569\n",
      "Accuracy of the network on the test set: 68 %\n",
      "Epoch [401]/[2000] running accumulative loss across all batches: 3.069\n",
      "Accuracy of the network on the test set: 62 %\n",
      "Epoch [421]/[2000] running accumulative loss across all batches: 1.685\n",
      "Accuracy of the network on the test set: 49 %\n",
      "Epoch [441]/[2000] running accumulative loss across all batches: 1.941\n",
      "Accuracy of the network on the test set: 38 %\n",
      "Epoch [461]/[2000] running accumulative loss across all batches: 2.697\n",
      "Accuracy of the network on the test set: 44 %\n",
      "Epoch [481]/[2000] running accumulative loss across all batches: 3.407\n",
      "Accuracy of the network on the test set: 46 %\n",
      "Epoch [501]/[2000] running accumulative loss across all batches: 4.253\n",
      "Accuracy of the network on the test set: 39 %\n",
      "Epoch [521]/[2000] running accumulative loss across all batches: 3.475\n",
      "Accuracy of the network on the test set: 70 %\n",
      "Epoch [541]/[2000] running accumulative loss across all batches: 3.413\n",
      "Accuracy of the network on the test set: 24 %\n",
      "Epoch [561]/[2000] running accumulative loss across all batches: 3.967\n",
      "Accuracy of the network on the test set: 62 %\n",
      "Epoch [581]/[2000] running accumulative loss across all batches: 3.735\n",
      "Accuracy of the network on the test set: 25 %\n",
      "Epoch [601]/[2000] running accumulative loss across all batches: 3.453\n",
      "Accuracy of the network on the test set: 69 %\n",
      "Epoch [621]/[2000] running accumulative loss across all batches: 5.451\n",
      "Accuracy of the network on the test set: 69 %\n",
      "Epoch [641]/[2000] running accumulative loss across all batches: 4.578\n",
      "Accuracy of the network on the test set: 45 %\n",
      "Epoch [661]/[2000] running accumulative loss across all batches: 4.865\n",
      "Accuracy of the network on the test set: 36 %\n",
      "Epoch [681]/[2000] running accumulative loss across all batches: 4.484\n",
      "Accuracy of the network on the test set: 63 %\n",
      "Epoch [701]/[2000] running accumulative loss across all batches: 3.895\n",
      "Accuracy of the network on the test set: 73 %\n",
      "Epoch [721]/[2000] running accumulative loss across all batches: 4.577\n",
      "Accuracy of the network on the test set: 69 %\n",
      "Epoch [741]/[2000] running accumulative loss across all batches: 4.951\n",
      "Accuracy of the network on the test set: 47 %\n",
      "Epoch [761]/[2000] running accumulative loss across all batches: 5.227\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [781]/[2000] running accumulative loss across all batches: 6.156\n",
      "Accuracy of the network on the test set: 68 %\n",
      "Epoch [801]/[2000] running accumulative loss across all batches: 5.666\n",
      "Accuracy of the network on the test set: 70 %\n",
      "Epoch [821]/[2000] running accumulative loss across all batches: 5.909\n",
      "Accuracy of the network on the test set: 40 %\n",
      "Epoch [841]/[2000] running accumulative loss across all batches: 5.295\n",
      "Accuracy of the network on the test set: 39 %\n",
      "Epoch [861]/[2000] running accumulative loss across all batches: 6.316\n",
      "Accuracy of the network on the test set: 59 %\n",
      "Epoch [881]/[2000] running accumulative loss across all batches: 5.927\n",
      "Accuracy of the network on the test set: 73 %\n",
      "Epoch [901]/[2000] running accumulative loss across all batches: 5.927\n",
      "Accuracy of the network on the test set: 75 %\n",
      "Epoch [921]/[2000] running accumulative loss across all batches: 5.979\n",
      "Accuracy of the network on the test set: 68 %\n",
      "Epoch [941]/[2000] running accumulative loss across all batches: 5.845\n",
      "Accuracy of the network on the test set: 73 %\n",
      "Epoch [961]/[2000] running accumulative loss across all batches: 6.520\n",
      "Accuracy of the network on the test set: 70 %\n",
      "Epoch [981]/[2000] running accumulative loss across all batches: 5.493\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1001]/[2000] running accumulative loss across all batches: 6.688\n",
      "Accuracy of the network on the test set: 64 %\n",
      "Epoch [1021]/[2000] running accumulative loss across all batches: 6.033\n",
      "Accuracy of the network on the test set: 73 %\n",
      "Epoch [1041]/[2000] running accumulative loss across all batches: 6.143\n",
      "Accuracy of the network on the test set: 68 %\n",
      "Epoch [1061]/[2000] running accumulative loss across all batches: 5.311\n",
      "Accuracy of the network on the test set: 65 %\n",
      "Epoch [1081]/[2000] running accumulative loss across all batches: 5.357\n",
      "Accuracy of the network on the test set: 70 %\n",
      "Epoch [1101]/[2000] running accumulative loss across all batches: 5.404\n",
      "Accuracy of the network on the test set: 69 %\n",
      "Epoch [1121]/[2000] running accumulative loss across all batches: 4.334\n",
      "Accuracy of the network on the test set: 68 %\n",
      "Epoch [1141]/[2000] running accumulative loss across all batches: 5.397\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1161]/[2000] running accumulative loss across all batches: 4.338\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1181]/[2000] running accumulative loss across all batches: 5.577\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1201]/[2000] running accumulative loss across all batches: 4.767\n",
      "Accuracy of the network on the test set: 69 %\n",
      "Epoch [1221]/[2000] running accumulative loss across all batches: 4.581\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1241]/[2000] running accumulative loss across all batches: 5.196\n",
      "Accuracy of the network on the test set: 70 %\n",
      "Epoch [1261]/[2000] running accumulative loss across all batches: 3.669\n",
      "Accuracy of the network on the test set: 71 %\n",
      "Epoch [1281]/[2000] running accumulative loss across all batches: 5.576\n",
      "Accuracy of the network on the test set: 55 %\n",
      "Epoch [1301]/[2000] running accumulative loss across all batches: 5.127\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1321]/[2000] running accumulative loss across all batches: 3.820\n",
      "Accuracy of the network on the test set: 70 %\n",
      "Epoch [1341]/[2000] running accumulative loss across all batches: 4.829\n",
      "Accuracy of the network on the test set: 68 %\n",
      "Epoch [1361]/[2000] running accumulative loss across all batches: 3.644\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1381]/[2000] running accumulative loss across all batches: 3.040\n",
      "Accuracy of the network on the test set: 71 %\n",
      "Epoch [1401]/[2000] running accumulative loss across all batches: 3.519\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1421]/[2000] running accumulative loss across all batches: 3.735\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1441]/[2000] running accumulative loss across all batches: 3.866\n",
      "Accuracy of the network on the test set: 73 %\n",
      "Epoch [1461]/[2000] running accumulative loss across all batches: 4.129\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1481]/[2000] running accumulative loss across all batches: 2.618\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1501]/[2000] running accumulative loss across all batches: 2.742\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1521]/[2000] running accumulative loss across all batches: 3.736\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1541]/[2000] running accumulative loss across all batches: 1.835\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1561]/[2000] running accumulative loss across all batches: 3.240\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1581]/[2000] running accumulative loss across all batches: 2.264\n",
      "Accuracy of the network on the test set: 59 %\n",
      "Epoch [1601]/[2000] running accumulative loss across all batches: 2.212\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1621]/[2000] running accumulative loss across all batches: 2.177\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1641]/[2000] running accumulative loss across all batches: 2.639\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1661]/[2000] running accumulative loss across all batches: 3.652\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1681]/[2000] running accumulative loss across all batches: 2.938\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1701]/[2000] running accumulative loss across all batches: 2.560\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1721]/[2000] running accumulative loss across all batches: 3.317\n",
      "Accuracy of the network on the test set: 89 %\n",
      "Epoch [1741]/[2000] running accumulative loss across all batches: 1.838\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1761]/[2000] running accumulative loss across all batches: 1.387\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1781]/[2000] running accumulative loss across all batches: 1.362\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1801]/[2000] running accumulative loss across all batches: 2.718\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1821]/[2000] running accumulative loss across all batches: 1.840\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1841]/[2000] running accumulative loss across all batches: 1.243\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1861]/[2000] running accumulative loss across all batches: 1.198\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1881]/[2000] running accumulative loss across all batches: 1.617\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1901]/[2000] running accumulative loss across all batches: 1.829\n",
      "Accuracy of the network on the test set: 91 %\n",
      "Epoch [1921]/[2000] running accumulative loss across all batches: 1.598\n",
      "Accuracy of the network on the test set: 74 %\n",
      "Epoch [1941]/[2000] running accumulative loss across all batches: 2.111\n",
      "Accuracy of the network on the test set: 90 %\n",
      "Epoch [1961]/[2000] running accumulative loss across all batches: 3.099\n",
      "Accuracy of the network on the test set: 62 %\n",
      "Epoch [1981]/[2000] running accumulative loss across all batches: 2.424\n"
     ]
    }
   ],
   "source": [
    "trainConcrete(modelConcrete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<ClampBackward1>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = F.gumbel_softmax(modelConcrete.concrete, tau=0.01, hard=True)\n",
    "mask = torch.sum(mask, axis=0)\n",
    "mask = torch.clamp(mask, min=0, max=1)\n",
    "mask"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
