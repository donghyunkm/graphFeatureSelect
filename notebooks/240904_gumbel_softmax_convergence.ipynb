{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-8.3289e-01, -3.7532e-01, -1.8049e+00, -1.1873e+00, -2.5143e+00,\n",
       "         -4.3779e-01, -1.5243e-01, -1.4132e+00, -1.3822e+00,  1.5159e+00,\n",
       "         -5.8740e-01, -1.1610e+00,  1.3189e+00,  6.2140e-01,  3.1834e-01,\n",
       "          1.1852e+00, -1.2456e+00, -1.2198e+00, -6.8095e-01, -9.3521e-01,\n",
       "          5.4539e-01,  1.8948e+00,  1.1441e+00,  1.0342e+00, -1.6833e-02,\n",
       "         -5.4095e-02,  7.5545e-01,  5.4895e-01,  6.6276e-01,  3.5680e-01,\n",
       "         -1.0821e+00,  3.3272e-01],\n",
       "        [-2.1817e+00, -4.9041e-01, -6.1976e-01, -5.7327e-01, -7.3050e-01,\n",
       "         -1.1674e+00,  1.5037e+00, -1.5489e+00, -5.5206e-01, -1.6347e+00,\n",
       "         -8.2686e-01, -1.0037e+00, -4.3762e-01,  7.7285e-01, -2.3297e+00,\n",
       "          5.0035e-01,  1.8790e+00, -4.8468e-01,  8.6224e-01, -3.5678e-01,\n",
       "          1.7236e+00,  4.6389e-04, -8.8696e-01, -1.9520e+00, -1.6304e+00,\n",
       "         -2.3636e+00, -1.5101e+00, -6.6748e-01, -4.4035e-01, -6.9837e-01,\n",
       "          3.1021e-01,  9.4109e-01],\n",
       "        [-9.4109e-01, -6.0933e-01, -1.3654e+00,  7.9148e-01,  7.2094e-01,\n",
       "          1.3299e+00,  6.6251e-01,  1.3816e+00,  3.8196e-01, -1.5854e+00,\n",
       "          7.4113e-01, -9.1090e-01, -7.7086e-01,  1.5528e-02, -2.3483e+00,\n",
       "          1.0985e+00,  2.2508e+00, -2.2492e-01,  3.2908e+00, -5.7913e-01,\n",
       "         -5.5108e-02, -5.7697e-01,  2.8936e-01,  9.4050e-01,  1.1878e+00,\n",
       "         -4.5717e-01,  3.3690e-02, -1.4357e-01,  1.1117e-01,  7.5746e-01,\n",
       "          4.3071e-01,  5.8288e-01],\n",
       "        [-6.3593e-01,  4.5975e-01,  1.6357e+00, -1.6227e-01,  2.8589e-01,\n",
       "          7.9058e-01,  1.0046e+00,  5.6947e-01,  1.6438e+00,  9.4212e-01,\n",
       "         -8.8010e-01, -1.3757e+00,  1.8172e-01,  1.7353e-01, -1.0760e+00,\n",
       "          4.9968e-01, -6.4070e-01,  1.2157e+00,  1.3229e+00, -1.5062e-01,\n",
       "         -3.4791e-01,  1.5854e-01,  1.3343e+00,  5.9390e-02,  1.1333e+00,\n",
       "          7.5495e-01, -1.4878e+00,  1.6766e+00, -1.9922e+00, -1.2043e+00,\n",
       "          9.3210e-01,  6.2277e-01],\n",
       "        [ 2.1404e+00, -2.4171e+00,  1.4029e+00,  2.4410e+00, -5.2188e-01,\n",
       "         -1.1198e-01, -7.6095e-01, -1.4888e+00, -7.2653e-01,  1.4951e+00,\n",
       "         -1.5375e+00,  7.6981e-02,  4.9234e-01,  7.5963e-01,  2.1640e-01,\n",
       "          1.2269e+00, -1.7747e-01,  3.2981e-01, -3.6870e-01, -2.7127e-01,\n",
       "          4.3145e-01, -2.8355e-01,  1.0660e+00, -2.7835e-01,  2.0519e-01,\n",
       "         -6.5896e-01, -9.2281e-01, -3.9207e-01,  7.3355e-01,  4.1195e-01,\n",
       "          1.6851e+00,  2.6159e-01],\n",
       "        [-2.7052e-01, -8.9819e-01, -1.1989e+00, -8.7640e-02,  3.6492e-01,\n",
       "          6.4603e-01,  1.0432e+00,  1.1166e+00,  1.1158e+00, -9.7927e-01,\n",
       "          6.1777e-01,  1.9591e+00, -3.1189e-01, -1.3037e+00, -6.4415e-02,\n",
       "          1.6772e+00,  5.2320e-01, -2.2293e-01, -1.0753e+00,  5.9009e-01,\n",
       "         -1.5412e+00, -1.7157e-01,  6.5073e-01,  9.2790e-01,  1.4266e+00,\n",
       "          1.7833e+00, -8.1069e-01, -1.1022e+00, -1.9187e-01,  1.9935e-01,\n",
       "          1.3002e-01,  4.4202e-01],\n",
       "        [ 4.1867e-03, -7.7557e-01, -4.0656e-01, -5.7801e-01,  1.0426e+00,\n",
       "         -2.1791e-01,  2.5238e-03,  5.0559e-01,  4.0329e-01, -1.2999e+00,\n",
       "          1.2094e+00, -4.3722e-01,  1.0957e+00, -1.8300e+00,  8.4226e-01,\n",
       "          8.5407e-01, -4.7416e-01, -1.1364e+00,  1.0112e+00,  1.2884e-01,\n",
       "         -4.9052e-01, -2.0352e-01,  3.6902e-01,  5.3528e-01, -5.3495e-01,\n",
       "         -4.9422e-01,  6.0479e-01, -2.4722e-01,  4.5207e-02,  8.0303e-01,\n",
       "          6.7398e-01, -3.0860e-01],\n",
       "        [-9.6729e-01,  7.6934e-01, -1.1142e+00, -9.5894e-01, -3.8278e-01,\n",
       "         -3.6778e-01,  6.2663e-02, -1.3751e+00, -5.8984e-01,  3.2043e-01,\n",
       "          1.2812e+00, -1.7863e+00,  3.7290e-01, -1.0932e+00, -1.1755e+00,\n",
       "          8.3878e-02,  5.0559e-01,  1.1354e+00, -2.0161e-01, -2.0880e+00,\n",
       "          1.4212e+00, -5.9615e-01,  2.7827e-01,  9.6191e-01, -1.4346e+00,\n",
       "          1.2233e+00, -1.5746e+00, -1.2669e+00,  1.0621e+00,  1.6808e+00,\n",
       "         -2.8361e-01,  4.2351e-01],\n",
       "        [ 1.0023e+00,  2.7068e-01,  8.9125e-01,  3.2857e-02, -1.8940e-01,\n",
       "         -9.7416e-01,  5.5338e-01,  1.5587e+00,  1.9781e-01,  7.9713e-01,\n",
       "         -9.3855e-02,  1.5721e+00,  7.1449e-01,  1.4648e+00, -7.9637e-01,\n",
       "          1.4252e+00, -8.8837e-02, -1.2188e-01,  1.3615e-01, -1.7589e+00,\n",
       "          8.3029e-01,  7.3431e-01, -1.3773e-01,  1.0127e+00,  7.6121e-01,\n",
       "          1.0343e+00,  1.1300e+00, -4.7900e-01,  1.6087e+00,  1.1081e+00,\n",
       "          9.1878e-01, -7.2788e-01],\n",
       "        [ 1.2323e+00,  3.0313e-01,  1.6027e+00,  1.4131e+00, -1.8929e+00,\n",
       "          8.9985e-01,  1.2456e+00,  1.9183e-01, -4.4799e-01, -1.1122e+00,\n",
       "          2.1381e+00,  1.1591e+00,  4.1764e-01,  5.5387e-01,  5.5169e-01,\n",
       "          8.0072e-01,  2.6680e-01,  1.2756e+00,  6.3119e-02,  2.1813e+00,\n",
       "          8.5967e-01, -2.8117e+00,  3.5334e-02,  1.1897e-01,  7.9190e-01,\n",
       "         -1.8124e+00, -2.7721e-01, -5.9232e-01, -4.5053e-01, -1.3072e+00,\n",
       "          7.7300e-01,  9.0982e-01],\n",
       "        [-3.3359e-01,  3.8328e-01,  3.6662e-01, -4.1784e-01, -3.2536e-01,\n",
       "          1.7644e+00,  1.0964e-01, -8.5044e-01, -1.1217e+00,  1.4956e+00,\n",
       "          6.6829e-02, -1.1842e+00,  1.9781e+00,  4.1444e-01, -1.0630e+00,\n",
       "          1.8070e+00,  6.5970e-01, -3.8757e-01,  2.9408e+00, -1.4228e-01,\n",
       "          1.2304e+00, -5.1065e-01, -2.5818e+00,  6.6547e-01, -1.3503e+00,\n",
       "          1.3596e+00, -1.9601e-01, -1.5876e+00,  9.9835e-01, -2.6895e-01,\n",
       "         -9.7406e-01,  3.3041e-01],\n",
       "        [ 1.2257e+00, -7.5463e-01,  1.9836e+00, -7.0124e-01, -5.3377e-01,\n",
       "          1.8452e+00, -1.3883e+00, -6.1092e-01, -9.7192e-01,  5.9658e-01,\n",
       "          2.1744e-01,  2.6782e-01, -2.1398e+00, -4.0643e-01, -2.8902e-01,\n",
       "          3.9369e-01,  1.3491e-01,  2.3895e+00, -1.4024e-01, -1.4750e+00,\n",
       "         -2.0939e-01,  8.3014e-01, -7.7540e-01,  6.0619e-01, -1.7018e+00,\n",
       "         -1.3996e+00,  8.3077e-02,  1.7509e+00,  1.3518e+00,  1.0078e-01,\n",
       "         -4.5036e-02,  2.3922e+00],\n",
       "        [ 1.5655e-01,  3.4102e-01, -4.7160e-01,  1.1707e+00, -1.3448e-01,\n",
       "          7.1320e-02,  5.9638e-01,  7.3382e-01, -5.7372e-01,  1.0613e+00,\n",
       "          1.4080e+00, -1.5428e+00,  4.2681e-01,  1.2393e+00, -1.5739e+00,\n",
       "         -1.3008e+00,  1.7084e+00,  4.0078e-01, -1.6367e-02,  2.5281e+00,\n",
       "         -4.0200e-01,  1.7493e-01,  1.3859e+00,  8.5359e-01, -4.7673e-01,\n",
       "         -1.9306e-01,  9.6623e-03,  2.3824e+00, -1.2319e-01, -5.3246e-01,\n",
       "          1.0850e+00, -6.7826e-02],\n",
       "        [-9.6397e-01,  1.6406e+00,  6.3096e-01,  2.1147e-02, -2.6446e-01,\n",
       "          2.0708e-01, -7.7789e-01, -2.5771e+00,  2.6409e+00, -1.6161e-01,\n",
       "         -2.8920e-02,  4.3959e-02,  6.7821e-01, -8.9984e-01,  4.4255e-01,\n",
       "          1.3330e-01,  7.6008e-01,  7.8864e-01,  1.0445e+00, -2.2074e+00,\n",
       "          1.6632e-01, -1.0054e+00,  1.3262e+00, -1.1327e+00, -7.8099e-01,\n",
       "         -1.1727e+00, -1.0132e-02, -6.7323e-02,  5.4785e-02,  4.4326e-01,\n",
       "          1.3602e+00,  8.3981e-02],\n",
       "        [ 1.3769e+00, -2.7912e-01,  1.2228e+00,  6.9799e-01,  1.9439e+00,\n",
       "         -8.1942e-01,  1.5706e-01, -3.6286e-01, -4.0177e-01, -3.6050e-01,\n",
       "         -1.4684e+00, -5.2634e-01, -1.2125e+00, -3.1240e-01,  1.7701e+00,\n",
       "         -2.3882e+00, -5.0698e-01, -1.0480e+00, -6.4057e-02, -4.7938e-01,\n",
       "         -2.0544e-01, -1.0858e+00, -2.0582e+00, -2.1242e-01,  4.0790e-01,\n",
       "          1.8455e-01,  5.1876e-01,  8.8941e-01,  2.1899e+00,  2.2246e+00,\n",
       "          7.6847e-01,  9.0979e-01],\n",
       "        [ 2.7192e-01,  6.8136e-01,  1.1777e-01, -5.3931e-03,  1.5987e+00,\n",
       "          1.8732e-01,  1.7172e-01,  2.3885e+00, -1.8363e+00, -2.8565e-01,\n",
       "          3.3696e-01, -1.3514e+00, -1.8571e-01,  1.8951e+00,  5.7222e-01,\n",
       "         -1.6965e+00, -3.0147e-01, -5.1217e-02, -9.4612e-01,  1.4283e+00,\n",
       "         -1.5239e+00, -1.9963e+00, -1.4867e-01, -1.0106e+00, -6.8766e-01,\n",
       "         -2.1045e+00, -2.5948e-02,  1.8165e-01,  2.2572e+00,  4.4753e-01,\n",
       "         -5.9089e-02, -8.5456e-01],\n",
       "        [ 2.1590e-01, -2.3812e+00,  8.8253e-01, -2.2414e-01,  2.9067e+00,\n",
       "         -1.3993e-01,  5.0339e-01,  1.7135e+00, -1.6126e+00, -1.4041e+00,\n",
       "         -2.9376e-01,  1.5559e-01,  2.3454e-01, -8.3872e-01,  7.2676e-01,\n",
       "          3.6346e-02, -3.5496e-01, -4.9454e-01, -4.1900e-02,  5.4191e-01,\n",
       "          2.4066e-01, -1.4628e-01, -3.1820e-02,  4.8818e-01, -2.0781e-01,\n",
       "          6.4574e-01, -7.0053e-01, -1.3550e+00, -1.6274e+00,  1.3562e+00,\n",
       "          2.7451e-01, -1.0162e+00],\n",
       "        [-8.9587e-01, -1.4124e+00,  1.9769e+00, -1.3184e+00,  3.9628e-01,\n",
       "          3.0484e-01, -8.5026e-01, -6.6473e-01, -2.1019e-02,  3.4691e-01,\n",
       "         -9.4918e-01,  9.1899e-01,  1.4696e+00, -5.4832e-02,  5.6907e-01,\n",
       "         -1.4099e+00,  2.0742e-01,  3.8234e-01,  1.5345e+00, -6.7867e-01,\n",
       "         -4.6192e-03, -8.1918e-01,  2.6914e-01, -2.7365e+00, -2.5642e-01,\n",
       "         -6.7464e-01, -8.6485e-01, -8.0980e-01, -2.3943e+00,  2.2736e-01,\n",
       "          2.5861e-01, -1.9306e-01],\n",
       "        [-1.1147e+00, -9.0803e-01,  5.1677e-02,  3.2837e-01, -5.3452e-01,\n",
       "          2.6125e-01, -8.1185e-02,  1.2014e+00,  6.7637e-01,  1.1898e+00,\n",
       "         -1.1814e-01, -2.7629e+00,  1.9705e-01, -1.3731e+00,  3.4378e-01,\n",
       "         -1.1589e+00,  1.0891e+00,  1.6919e+00,  2.2929e-01, -8.5299e-01,\n",
       "         -2.2876e+00,  5.4637e-01,  6.6381e-01, -2.3946e-01,  1.3197e+00,\n",
       "          1.0040e+00, -8.3250e-01, -6.1726e-01, -5.8612e-01, -1.3759e+00,\n",
       "         -1.3010e-01,  7.2690e-02],\n",
       "        [ 4.6742e-01,  3.4361e-03,  7.0300e-02, -9.2290e-01,  6.7449e-01,\n",
       "         -1.6721e-01,  1.6917e-01, -1.2987e+00, -1.2589e+00, -1.1319e+00,\n",
       "         -6.0094e-02, -3.5075e-01, -9.2900e-01, -2.0791e-01,  1.0199e+00,\n",
       "          1.2278e+00,  1.3147e+00,  4.2638e+00, -1.9171e+00,  1.1885e+00,\n",
       "         -1.5126e+00, -8.2450e-01, -1.0112e+00,  1.2170e+00,  7.2280e-01,\n",
       "         -3.7264e-01, -1.8236e-01, -1.2340e+00,  1.6167e-01,  1.7012e+00,\n",
       "         -1.4279e+00,  3.6502e-01]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn(20, 32)\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.3276e-03, 4.1848e-03, 1.7100e-03, 1.4884e-03, 3.8685e-04, 4.9372e-03,\n",
       "         1.3238e-02, 3.4188e-03, 8.9195e-04, 3.7121e-02, 1.4080e-02, 6.2456e-03,\n",
       "         2.8920e-02, 5.6853e-02, 3.3346e-03, 9.7475e-02, 3.2392e-03, 8.5327e-03,\n",
       "         6.6554e-03, 1.3953e-02, 2.0443e-02, 1.6846e-01, 2.2106e-01, 1.0588e-02,\n",
       "         2.3714e-02, 1.8931e-02, 1.9513e-02, 2.8238e-02, 1.7648e-02, 1.0062e-01,\n",
       "         9.2759e-03, 4.5518e-02],\n",
       "        [2.3446e-03, 1.0397e-02, 9.5342e-02, 1.9514e-03, 1.5281e-03, 5.5212e-04,\n",
       "         4.7910e-02, 7.3021e-04, 2.9806e-03, 1.2263e-03, 2.3316e-03, 3.9776e-03,\n",
       "         5.1823e-03, 2.0586e-02, 6.4888e-04, 4.6690e-03, 4.0825e-02, 9.1261e-03,\n",
       "         3.8768e-02, 4.2723e-03, 7.7592e-02, 1.3074e-02, 1.2576e-03, 2.5044e-03,\n",
       "         4.6275e-04, 1.3893e-03, 6.3583e-04, 2.0254e-03, 4.6681e-03, 3.8385e-02,\n",
       "         3.8080e-01, 1.8185e-01],\n",
       "        [7.2437e-03, 1.6250e-03, 1.4832e-03, 3.9936e-01, 2.0402e-02, 3.1000e-02,\n",
       "         1.1732e-02, 2.7012e-02, 9.9260e-03, 1.1781e-03, 1.9539e-02, 6.1284e-04,\n",
       "         1.7680e-03, 1.3939e-02, 6.4775e-03, 3.4107e-03, 4.1588e-02, 1.8836e-02,\n",
       "         2.1678e-01, 1.6450e-02, 6.8533e-03, 3.1329e-03, 2.1266e-03, 1.9908e-02,\n",
       "         3.0651e-02, 1.1656e-03, 2.6634e-02, 2.2195e-03, 2.6941e-03, 3.1457e-03,\n",
       "         1.7268e-02, 3.3845e-02],\n",
       "        [1.5185e-03, 8.7111e-02, 1.6368e-02, 1.1159e-03, 1.4992e-03, 2.8448e-03,\n",
       "         3.9774e-03, 8.6040e-03, 3.5949e-03, 6.8928e-03, 1.4569e-03, 1.4651e-03,\n",
       "         1.8597e-03, 1.0247e-01, 4.4276e-04, 6.9271e-03, 1.8007e-03, 2.9963e-03,\n",
       "         3.7027e-02, 2.7596e-03, 6.7391e-02, 5.5580e-03, 2.2023e-01, 9.2913e-04,\n",
       "         1.2023e-03, 2.0728e-01, 9.5665e-04, 2.9881e-02, 2.9944e-04, 6.8112e-04,\n",
       "         1.6428e-01, 8.5691e-03],\n",
       "        [1.8469e-02, 1.6667e-04, 1.9714e-02, 2.3900e-02, 2.5890e-03, 4.2504e-03,\n",
       "         4.4344e-03, 1.6484e-03, 7.7361e-04, 1.6319e-02, 9.6172e-04, 1.9400e-02,\n",
       "         3.1159e-03, 3.1602e-02, 2.8501e-02, 3.6127e-02, 4.6795e-02, 7.7954e-03,\n",
       "         9.0407e-03, 3.6951e-03, 3.6185e-03, 1.3507e-02, 2.4823e-01, 3.5025e-03,\n",
       "         3.6179e-01, 2.0326e-03, 2.2215e-02, 5.2800e-03, 1.0132e-02, 6.7483e-03,\n",
       "         3.7990e-02, 5.6611e-03],\n",
       "        [3.6501e-04, 5.6427e-04, 1.0191e-04, 1.6172e-03, 9.9265e-04, 5.3369e-04,\n",
       "         9.0326e-01, 2.4591e-03, 7.0980e-04, 9.3723e-04, 3.7378e-03, 1.7848e-03,\n",
       "         2.3332e-03, 1.1399e-04, 1.5213e-03, 2.9622e-02, 1.1845e-03, 7.4220e-04,\n",
       "         7.0025e-04, 4.8023e-03, 3.3090e-03, 2.9962e-03, 5.6155e-04, 5.4331e-03,\n",
       "         8.9298e-04, 1.1087e-03, 3.7216e-04, 9.2922e-05, 7.7463e-04, 2.0842e-02,\n",
       "         4.5014e-03, 1.0340e-03],\n",
       "        [8.3845e-04, 2.2633e-04, 6.4431e-03, 1.1814e-03, 1.8594e-03, 3.6144e-03,\n",
       "         2.9507e-03, 3.5969e-03, 1.5125e-02, 6.6203e-04, 1.1213e-02, 2.3458e-04,\n",
       "         7.2936e-02, 5.1855e-04, 4.2580e-03, 2.1296e-03, 7.1320e-03, 2.4918e-04,\n",
       "         6.1098e-02, 7.1593e-03, 4.3734e-04, 5.0928e-03, 1.8159e-03, 2.0490e-03,\n",
       "         5.8943e-03, 3.9771e-04, 7.1269e-01, 2.5161e-04, 4.3159e-03, 1.2311e-03,\n",
       "         6.1848e-02, 5.5185e-04],\n",
       "        [3.7314e-03, 8.6160e-03, 1.2642e-03, 1.6644e-03, 1.2107e-02, 1.4776e-02,\n",
       "         1.5517e-02, 1.9027e-03, 1.3015e-03, 1.0230e-02, 5.5955e-02, 1.6171e-02,\n",
       "         7.8175e-03, 2.5303e-03, 3.9582e-03, 4.6484e-02, 1.4308e-02, 1.0686e-01,\n",
       "         2.6060e-02, 1.8621e-03, 2.1143e-02, 2.0036e-02, 1.5725e-02, 7.9244e-02,\n",
       "         5.6685e-04, 1.1924e-01, 1.1775e-03, 5.5201e-03, 1.1558e-02, 1.1585e-01,\n",
       "         8.4782e-03, 2.4834e-01],\n",
       "        [5.9015e-03, 1.1692e-02, 5.4633e-03, 8.3768e-03, 4.0880e-03, 5.3207e-04,\n",
       "         2.5785e-02, 1.9747e-02, 5.1010e-01, 7.1431e-03, 9.6566e-03, 3.6691e-02,\n",
       "         3.5860e-03, 1.0574e-01, 1.4980e-03, 8.8469e-02, 1.4470e-02, 4.3993e-04,\n",
       "         5.0523e-03, 8.6107e-04, 8.1165e-03, 1.7519e-02, 1.2276e-03, 1.7275e-03,\n",
       "         3.3814e-02, 2.4298e-03, 4.3446e-02, 1.2609e-03, 1.3549e-02, 3.5105e-03,\n",
       "         7.0300e-03, 1.0773e-03],\n",
       "        [1.0487e-02, 3.9798e-02, 9.1464e-02, 8.3538e-02, 1.0931e-03, 5.6347e-02,\n",
       "         2.2781e-02, 6.2597e-03, 1.0171e-02, 2.6037e-03, 1.4186e-01, 1.2756e-01,\n",
       "         3.7260e-03, 1.1906e-02, 4.7283e-03, 4.5777e-02, 6.8145e-03, 5.9257e-02,\n",
       "         4.2659e-03, 4.8745e-02, 1.4995e-02, 3.0204e-04, 4.6017e-03, 1.0929e-02,\n",
       "         8.1990e-03, 9.0133e-04, 8.0228e-03, 2.5794e-02, 2.6780e-02, 1.3964e-03,\n",
       "         6.2512e-02, 5.6384e-02],\n",
       "        [3.2655e-03, 4.1873e-03, 8.9052e-03, 4.2996e-02, 1.4286e-02, 6.3494e-02,\n",
       "         9.1068e-02, 1.3299e-02, 9.2102e-03, 1.5390e-02, 4.8242e-03, 2.2284e-01,\n",
       "         1.8411e-02, 8.8497e-03, 1.9210e-03, 2.4755e-02, 3.1011e-02, 5.2762e-03,\n",
       "         1.5984e-01, 6.1827e-02, 3.1593e-02, 3.5406e-03, 7.7312e-04, 4.4061e-02,\n",
       "         7.5061e-04, 4.7268e-02, 2.0366e-02, 1.8570e-03, 8.6970e-03, 9.6825e-03,\n",
       "         1.8886e-03, 2.3866e-02],\n",
       "        [1.6873e-02, 2.3526e-03, 3.4489e-02, 1.3168e-03, 1.5911e-03, 1.2707e-02,\n",
       "         9.6045e-04, 4.6044e-03, 2.1957e-03, 5.9116e-02, 1.9731e-02, 3.2758e-02,\n",
       "         2.9314e-03, 8.0652e-03, 1.7136e-03, 5.1781e-03, 8.2883e-03, 2.3933e-02,\n",
       "         1.7243e-03, 9.3143e-04, 3.9037e-03, 1.0640e-02, 4.1344e-03, 5.8382e-01,\n",
       "         3.0823e-04, 1.5401e-03, 1.8348e-03, 5.0355e-03, 1.1588e-02, 6.4384e-03,\n",
       "         1.0985e-01, 1.9440e-02],\n",
       "        [8.2786e-03, 1.0996e-02, 3.8158e-03, 1.2763e-02, 2.1169e-02, 8.2669e-03,\n",
       "         9.8248e-03, 1.5213e-02, 8.1315e-03, 2.9185e-02, 1.2030e-02, 8.3479e-04,\n",
       "         1.0103e-02, 1.2126e-01, 4.8274e-04, 7.7341e-03, 5.0160e-02, 2.8273e-02,\n",
       "         7.3144e-03, 8.5982e-02, 2.9974e-02, 3.7044e-02, 8.8148e-02, 5.2242e-03,\n",
       "         8.5755e-03, 1.8819e-01, 2.3063e-02, 5.4920e-02, 1.7756e-03, 1.6319e-03,\n",
       "         9.4867e-02, 1.4765e-02],\n",
       "        [3.8945e-03, 6.8147e-01, 4.0569e-02, 2.9133e-03, 3.2473e-03, 7.1293e-03,\n",
       "         5.4648e-03, 2.7171e-04, 1.4959e-02, 5.2994e-03, 4.3486e-04, 1.0643e-02,\n",
       "         3.6904e-02, 2.2164e-03, 6.9344e-03, 6.9193e-04, 3.9787e-02, 8.6886e-03,\n",
       "         3.4748e-02, 1.1094e-04, 1.7845e-02, 2.7706e-04, 4.1579e-03, 2.3215e-04,\n",
       "         1.8538e-03, 1.0083e-02, 5.0133e-04, 2.3971e-02, 1.0405e-03, 1.9753e-02,\n",
       "         2.6012e-03, 1.1304e-02],\n",
       "        [4.0992e-03, 1.5711e-02, 3.0148e-03, 7.5447e-03, 2.6533e-01, 4.2671e-03,\n",
       "         3.2609e-03, 4.5181e-03, 2.9079e-03, 3.0432e-03, 3.9422e-04, 3.5242e-03,\n",
       "         1.1371e-03, 1.7538e-03, 2.0272e-02, 9.0315e-05, 1.2907e-03, 8.5730e-04,\n",
       "         9.7588e-03, 1.6195e-03, 2.3999e-02, 1.2904e-03, 3.5309e-04, 2.2723e-03,\n",
       "         6.8734e-03, 7.1776e-03, 4.7046e-02, 2.2586e-03, 1.2519e-02, 5.3112e-01,\n",
       "         4.9104e-03, 5.7823e-03],\n",
       "        [1.0011e-02, 1.7238e-03, 3.4805e-02, 2.1416e-02, 3.7658e-01, 3.7311e-03,\n",
       "         9.0327e-03, 1.7137e-02, 4.4321e-04, 2.2582e-01, 6.3986e-02, 7.9470e-04,\n",
       "         1.4783e-03, 4.0221e-02, 1.4058e-02, 2.5464e-04, 5.4727e-03, 7.0353e-02,\n",
       "         1.4270e-03, 1.2524e-02, 1.9004e-04, 8.4196e-04, 2.8509e-03, 9.7259e-03,\n",
       "         1.7818e-03, 3.8551e-04, 1.5499e-03, 8.3710e-03, 5.2020e-02, 2.5016e-03,\n",
       "         4.6356e-03, 3.8788e-03],\n",
       "        [6.3386e-02, 6.2436e-04, 1.9766e-02, 8.9090e-03, 6.5215e-01, 3.9521e-03,\n",
       "         5.8348e-03, 2.4440e-02, 8.1654e-04, 7.8981e-04, 2.0678e-02, 1.2299e-03,\n",
       "         1.4165e-02, 1.5275e-03, 1.0743e-02, 1.6128e-02, 2.5460e-03, 1.0534e-02,\n",
       "         1.2857e-02, 7.1879e-03, 1.9060e-02, 2.1805e-03, 4.8642e-03, 7.9788e-03,\n",
       "         9.2352e-04, 2.5847e-02, 3.5603e-02, 3.2107e-04, 2.3839e-03, 1.4723e-02,\n",
       "         5.7919e-03, 2.0588e-03],\n",
       "        [4.7911e-03, 2.5760e-02, 4.5685e-02, 2.7375e-03, 2.1567e-02, 3.4617e-03,\n",
       "         1.4883e-02, 9.4389e-03, 4.4066e-03, 6.5326e-03, 1.4074e-03, 5.4798e-03,\n",
       "         1.3668e-02, 3.7145e-02, 1.0705e-02, 7.8989e-04, 5.7524e-03, 9.4161e-02,\n",
       "         4.5905e-01, 5.1479e-03, 3.5741e-02, 4.2982e-03, 3.4612e-02, 2.7155e-04,\n",
       "         1.5414e-02, 1.8390e-03, 6.7375e-03, 3.6882e-02, 9.7815e-05, 4.1662e-02,\n",
       "         4.1595e-02, 8.2842e-03],\n",
       "        [9.3225e-04, 3.7763e-03, 1.2207e-02, 1.4682e-02, 6.2837e-03, 5.2355e-02,\n",
       "         3.4940e-02, 1.5852e-02, 1.5530e-01, 4.6897e-02, 9.8558e-03, 3.1060e-03,\n",
       "         7.6591e-03, 7.7775e-04, 1.9964e-01, 1.5327e-02, 1.9788e-02, 3.8865e-02,\n",
       "         7.9803e-02, 7.6189e-03, 2.3453e-03, 9.8018e-03, 9.9103e-02, 1.2386e-02,\n",
       "         4.1017e-02, 6.9932e-02, 4.7781e-03, 3.6219e-03, 1.4146e-02, 2.9182e-03,\n",
       "         8.6257e-03, 5.6647e-03],\n",
       "        [2.4548e-03, 9.9351e-03, 2.0091e-02, 1.9363e-03, 3.2605e-01, 5.9910e-04,\n",
       "         1.6853e-02, 6.1291e-04, 2.0391e-03, 1.5039e-03, 3.6334e-03, 8.8201e-04,\n",
       "         1.6644e-02, 2.1606e-02, 2.1358e-02, 5.0724e-02, 9.3738e-03, 8.2828e-02,\n",
       "         1.9322e-04, 9.9828e-02, 6.9606e-04, 1.6795e-03, 1.4653e-03, 5.0013e-03,\n",
       "         1.8885e-02, 9.3832e-04, 1.9561e-03, 2.6069e-03, 2.7247e-02, 2.3851e-01,\n",
       "         3.5097e-04, 1.1526e-02]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample soft categorical using reparametrization trick:\n",
    "F.gumbel_softmax(logits, tau=1, hard=False)\n",
    "# Sample hard categorical using \"Straight-through\" trick:\n",
    "F.gumbel_softmax(logits, tau=1, hard=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits: tensor([ 1.1916, -0.8708, -0.2815,  1.2108], requires_grad=True)\n",
      "x: tensor([0.6098, 0.0123, 0.0945, 0.2834], grad_fn=<SoftmaxBackward0>)\n",
      "grad: tensor([ 0.1811, -0.0110, -0.0693, -0.1008])\n"
     ]
    }
   ],
   "source": [
    "logits = torch.randn(4, requires_grad=True)\n",
    "print(\"logits:\", logits)\n",
    "x = F.gumbel_softmax(logits, tau=1, hard=False)\n",
    "print(\"x:\", x)\n",
    "\n",
    "ground_truth = torch.tensor([1.0, 1.0, 1.0, 1.0])\n",
    "loss = torch.sum((x - ground_truth) ** 2)\n",
    "# loss.backward(gradient = torch.tensor([1., 1., 1., 1.]))\n",
    "loss.backward()\n",
    "\n",
    "# print the gradient using .grad\n",
    "print(\"grad:\", logits.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# logits = torch.randn(4, requires_grad=True)\n",
    "\n",
    "ground_truth = torch.tensor([1.0, 0, 0, 0])\n",
    "loss_fn = nn.MSELoss()\n",
    "kl_loss = nn.KLDivLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gumbel_m(nn.Module):\n",
    "    def __init__(self, temp):\n",
    "        super().__init__()\n",
    "        self.logits = nn.Parameter(torch.randn(4))\n",
    "        self.temp = temp\n",
    "\n",
    "    def forward(self):\n",
    "        x = F.gumbel_softmax(self.logits, tau=self.temp, hard=False)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits tensor([-0.9917,  0.9721,  0.2368,  0.0959])\n"
     ]
    }
   ],
   "source": [
    "model = gumbel_m()\n",
    "print(\"initial distribution\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0455, 0.1650, 0.4725, 0.3171], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.0056, 0.1103, 0.8186, 0.0655], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.5409, 0.3669, 0.0435, 0.0487], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.9591, 0.0083, 0.0061, 0.0265], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.8960, 0.0160, 0.0122, 0.0759], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.8264, 0.1411, 0.0104, 0.0221], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.8719, 0.0919, 0.0037, 0.0325], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.9950, 0.0021, 0.0011, 0.0019], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.9486e-01, 7.5520e-04, 2.1441e-03, 2.2442e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.9627, 0.0147, 0.0207, 0.0019], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.9516e-01, 3.2243e-04, 3.0928e-03, 1.4236e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using MAE loss, distribution converges to target\n",
    "for epoch in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    out = model()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(out)\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(out, ground_truth)\n",
    "    # print(loss)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits tensor([-2.1847,  0.4396, -1.3671,  0.9186])\n"
     ]
    }
   ],
   "source": [
    "model2 = gumbel_m()\n",
    "print(\"initial distribution\")\n",
    "for name, param in model2.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer2 = torch.optim.Adam(model2.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.6275, 0.1467, 0.0493, 0.1765], grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/users1/dkim195/miniconda3/envs/env/lib/python3.11/site-packages/torch/nn/functional.py:2994: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0644, 0.1758, 0.3054, 0.4544], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.1309, 0.0326, 0.0032, 0.8333], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.9336e-01, 4.6259e-04, 8.4621e-04, 5.3346e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.8771, 0.1093, 0.0029, 0.0107], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.9648e-01, 5.8527e-04, 2.6101e-03, 3.2602e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.9432, 0.0389, 0.0031, 0.0148], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.9204e-01, 1.0175e-03, 1.4226e-04, 6.8028e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.8124e-01, 9.5598e-04, 6.4678e-04, 1.7162e-02],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.9915e-01, 3.7352e-04, 5.5028e-05, 4.1997e-04],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([9.9437e-01, 2.8967e-04, 7.6745e-05, 5.2586e-03],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Using KL Divergence loss, distribution converges to target\n",
    "for epoch in range(10001):\n",
    "    optimizer2.zero_grad()\n",
    "    out = model2()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(out)\n",
    "    loss = kl_loss(out, ground_truth)\n",
    "    # loss = loss_fn(out, ground_truth)\n",
    "    # print(loss)\n",
    "    loss.backward()\n",
    "    optimizer2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits tensor([ 2.1702, -0.0610, -0.4752,  0.3097])\n",
      "tensor([0.3114, 0.2445, 0.1994, 0.2448], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.3733, 0.1974, 0.2169, 0.2124], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.4204, 0.2068, 0.1887, 0.1842], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.4094, 0.1787, 0.2018, 0.2101], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.4838, 0.2018, 0.1484, 0.1661], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.5099, 0.1537, 0.1714, 0.1649], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.5613, 0.1574, 0.1481, 0.1332], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.5678, 0.1470, 0.1446, 0.1406], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.6395, 0.1088, 0.1241, 0.1276], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.6479, 0.1155, 0.0983, 0.1383], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0.7288, 0.0898, 0.0912, 0.0902], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing high temp\n",
    "# gradients pass back so converges okay\n",
    "\n",
    "model3 = gumbel_m(temp=10)\n",
    "print(\"initial distribution\")\n",
    "for name, param in model3.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer3 = torch.optim.Adam(model3.parameters())\n",
    "\n",
    "# Using MLE loss, distribution converges to target\n",
    "for epoch in range(10001):\n",
    "    optimizer3.zero_grad()\n",
    "    out = model3()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(out)\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(out, ground_truth)\n",
    "    # print(loss)\n",
    "    loss.backward()\n",
    "    optimizer3.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits tensor([ 0.7105,  1.7994, -1.0446, -1.4619])\n",
      "tensor([1.0000e+00, 1.6669e-32, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1.0000e+00, 7.3876e-10, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([5.6302e-19, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1.7870e-04, 9.9982e-01, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([7.3521e-14, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1.7495e-11, 1.0000e+00, 0.0000e+00, 0.0000e+00],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1.0000e+00, 0.0000e+00, 4.7773e-21, 0.0000e+00],\n",
      "       grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1., 0., 0., 0.], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing low temp\n",
    "# gradients aren't the best so convergence path is weird\n",
    "\n",
    "model4 = gumbel_m(temp=0.01)\n",
    "print(\"initial distribution\")\n",
    "for name, param in model4.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer4 = torch.optim.Adam(model4.parameters())\n",
    "\n",
    "# Using MLE loss, distribution converges to target\n",
    "for epoch in range(10001):\n",
    "    optimizer4.zero_grad()\n",
    "    out = model4()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(out)\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(out, ground_truth)\n",
    "    # print(loss)\n",
    "    loss.backward()\n",
    "    optimizer4.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits tensor([-1.0412, -1.0363,  0.3127,  1.0133])\n",
      "tensor([0., 0., 0., 1.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 1., 0., 0.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 1., 0.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1.], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([0., 0., 0., 1.], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing very low temp\n",
    "# no gradients so does not converge\n",
    "\n",
    "model5 = gumbel_m(temp=1e-10)\n",
    "print(\"initial distribution\")\n",
    "for name, param in model5.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer5 = torch.optim.Adam(model5.parameters())\n",
    "\n",
    "# Using MLE loss, distribution converges to target\n",
    "for epoch in range(10001):\n",
    "    optimizer5.zero_grad()\n",
    "    out = model5()\n",
    "    if epoch % 1000 == 0:\n",
    "        print(out)\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(out, ground_truth)\n",
    "    # print(loss)\n",
    "    loss.backward()\n",
    "    optimizer5.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32190356, 0.31582075, 0.36227572], dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp1 = []\n",
    "for _ in range(10000):\n",
    "    temp1.append(F.gumbel_softmax(torch.Tensor([0.3, 0.3, 0.4]), tau=0.01, hard=False))\n",
    "np.mean(np.array(temp1), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3439453/156050755.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  F.softmax(torch.Tensor([0.3, 0.3, 0.4]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.3220, 0.3220, 0.3559])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(torch.Tensor([0.3, 0.3, 0.4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mask_learning(nn.Module):\n",
    "    def __init__(self, temp):\n",
    "        super().__init__()\n",
    "        self.logits = nn.Parameter(torch.randn(3))\n",
    "        self.temp = temp\n",
    "        self.lin1 = nn.Linear(3, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = F.gumbel_softmax(self.logits, tau=self.temp, hard=True)\n",
    "        y_pred = self.lin1(mask * x)\n",
    "        return y_pred\n",
    "\n",
    "    def softmax(self):\n",
    "        return F.softmax(self.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1000, 3)\n",
    "y = x[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial distribution\n",
      "logits tensor([ 0.1570, -0.2804,  1.9538])\n",
      "lin1.weight tensor([[ 0.2386, -0.1962, -0.1054],\n",
      "        [-0.0572, -0.0825, -0.3542]])\n",
      "lin1.bias tensor([-0.4225,  0.4419])\n"
     ]
    }
   ],
   "source": [
    "model = mask_learning(temp=1)\n",
    "print(\"initial distribution\")\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3439453/2159838833.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1303, 0.0841, 0.7856], grad_fn=<SoftmaxBackward0>)\n",
      "1.617917776107788\n",
      "tensor([0.0991, 0.0457, 0.8551], grad_fn=<SoftmaxBackward0>)\n",
      "0.6916490197181702\n",
      "tensor([0.0280, 0.0176, 0.9545], grad_fn=<SoftmaxBackward0>)\n",
      "0.5084733366966248\n",
      "tensor([0.0193, 0.0189, 0.9618], grad_fn=<SoftmaxBackward0>)\n",
      "0.4982832074165344\n",
      "tensor([0.0182, 0.0281, 0.9537], grad_fn=<SoftmaxBackward0>)\n",
      "0.4982263445854187\n",
      "tensor([0.0200, 0.0675, 0.9125], grad_fn=<SoftmaxBackward0>)\n",
      "0.4982301592826843\n",
      "tensor([0.0197, 0.1803, 0.8001], grad_fn=<SoftmaxBackward0>)\n",
      "0.5179229974746704\n",
      "tensor([0.0164, 0.3316, 0.6520], grad_fn=<SoftmaxBackward0>)\n",
      "0.4983106255531311\n",
      "tensor([0.0122, 0.4432, 0.5446], grad_fn=<SoftmaxBackward0>)\n",
      "0.5094834566116333\n",
      "tensor([0.0093, 0.4584, 0.5323], grad_fn=<SoftmaxBackward0>)\n",
      "1.011027216911316\n",
      "tensor([0.0071, 0.4912, 0.5017], grad_fn=<SoftmaxBackward0>)\n",
      "0.49832069873809814\n"
     ]
    }
   ],
   "source": [
    "# Using MAE loss, distribution converges to target\n",
    "for epoch in range(10001):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x)\n",
    "    # loss = kl_loss(out, ground_truth)\n",
    "    loss = loss_fn(out, y)\n",
    "    if epoch % 1000 == 0:\n",
    "        print(model.softmax())\n",
    "        print(float(loss))\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "graphFeatureSelect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
